{"type":"system","subtype":"init","cwd":"/Users/emmayu/crqbench/projects/clusterfuzz/src/python/bot/tasks","session_id":"6939fe31-d677-4c31-9307-4891924f760e","tools":["Bash","Read","Edit"],"mcp_servers":[],"model":"claude-opus-4-20250514","permissionMode":"default","slash_commands":["keybindings-help","compact","context","cost","init","pr-comments","release-notes","review","security-review"],"apiKeySource":"/login managed key","claude_code_version":"2.1.23","output_style":"default","agents":["Bash","general-purpose","statusline-setup","Explore","Plan"],"skills":["keybindings-help"],"plugins":[],"uuid":"dec7c5da-5f96-4285-a2bb-e2c7b52be633"}
{"type":"assistant","message":{"model":"claude-opus-4-20250514","id":"msg_011zcYdU46ta2Hj7bEh2yy1y","type":"message","role":"assistant","content":[{"type":"text","text":"I'll help you understand whether the `if not build_directory` check is needed. Let me first look at the relevant code to see the context."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":6911,"cache_read_input_tokens":0,"cache_creation":{"ephemeral_5m_input_tokens":6911,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"6939fe31-d677-4c31-9307-4891924f760e","uuid":"e882636c-22df-41e0-937a-a88aa7ce5d05"}
{"type":"assistant","message":{"model":"claude-opus-4-20250514","id":"msg_011zcYdU46ta2Hj7bEh2yy1y","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01PevK2iydoeWjNL1oA8Bngk","name":"Bash","input":{"command":"find . -type f -name \"*.py\" | grep -E \"(build|directory)\" | head -20","description":"Search for Python files that might contain build_directory references"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":6911,"cache_read_input_tokens":0,"cache_creation":{"ephemeral_5m_input_tokens":6911,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"6939fe31-d677-4c31-9307-4891924f760e","uuid":"b864cbdd-bfe7-4a29-9107-7bd8b2a8d54a"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01PevK2iydoeWjNL1oA8Bngk","type":"tool_result","content":"","is_error":false}]},"parent_tool_use_id":null,"session_id":"6939fe31-d677-4c31-9307-4891924f760e","uuid":"70c27934-786c-464f-b21a-440660c22ff3","tool_use_result":{"stdout":"","stderr":"","interrupted":false,"isImage":false}}
{"type":"assistant","message":{"model":"claude-opus-4-20250514","id":"msg_01QjBLek4Bamm9PQzE62e6c9","type":"message","role":"assistant","content":[{"type":"text","text":"Let me search more broadly for files that might contain this check:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":7,"cache_creation_input_tokens":156,"cache_read_input_tokens":6911,"cache_creation":{"ephemeral_5m_input_tokens":156,"ephemeral_1h_input_tokens":0},"output_tokens":7,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"6939fe31-d677-4c31-9307-4891924f760e","uuid":"bce0a2ce-aa4a-4989-bfb5-784c2ec7618b"}
{"type":"assistant","message":{"model":"claude-opus-4-20250514","id":"msg_01QjBLek4Bamm9PQzE62e6c9","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01YLEnfqvm1NjBCVYmQ7ukbv","name":"Bash","input":{"command":"grep -r \"if not build_directory\" . --include=\"*.py\" | head -20","description":"Search for the specific 'if not build_directory' check in Python files"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":7,"cache_creation_input_tokens":156,"cache_read_input_tokens":6911,"cache_creation":{"ephemeral_5m_input_tokens":156,"ephemeral_1h_input_tokens":0},"output_tokens":7,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"6939fe31-d677-4c31-9307-4891924f760e","uuid":"cb1301d9-f257-4adc-885b-f73a915ff22a"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01YLEnfqvm1NjBCVYmQ7ukbv","type":"tool_result","content":"","is_error":false}]},"parent_tool_use_id":null,"session_id":"6939fe31-d677-4c31-9307-4891924f760e","uuid":"2436f74b-24ba-412a-9515-e2ceb4efa9a5","tool_use_result":{"stdout":"","stderr":"","interrupted":false,"isImage":false}}
{"type":"assistant","message":{"model":"claude-opus-4-20250514","id":"msg_0186CBRVvDAkwbbzHWKzDsfp","type":"message","role":"assistant","content":[{"type":"text","text":"Let me check the current directory and look for Python files:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":7,"cache_creation_input_tokens":144,"cache_read_input_tokens":7067,"cache_creation":{"ephemeral_5m_input_tokens":144,"ephemeral_1h_input_tokens":0},"output_tokens":3,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"6939fe31-d677-4c31-9307-4891924f760e","uuid":"6a8726c3-994e-41c4-bf79-c595d8566fdd"}
{"type":"assistant","message":{"model":"claude-opus-4-20250514","id":"msg_0186CBRVvDAkwbbzHWKzDsfp","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_013tT3EF8gc289db3Nm1esbs","name":"Bash","input":{"command":"ls -la *.py","description":"List Python files in current directory"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":7,"cache_creation_input_tokens":144,"cache_read_input_tokens":7067,"cache_creation":{"ephemeral_5m_input_tokens":144,"ephemeral_1h_input_tokens":0},"output_tokens":3,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"6939fe31-d677-4c31-9307-4891924f760e","uuid":"04230f2f-e790-4a2a-8d4d-7052e361928f"}
{"type":"user","message":{"role":"user","content":[{"type":"tool_result","content":"This command requires approval","is_error":true,"tool_use_id":"toolu_013tT3EF8gc289db3Nm1esbs"}]},"parent_tool_use_id":null,"session_id":"6939fe31-d677-4c31-9307-4891924f760e","uuid":"066eec53-84d8-42ab-abf8-ffd0426b2321","tool_use_result":"Error: This command requires approval"}
{"type":"assistant","message":{"model":"claude-opus-4-20250514","id":"msg_01A2oBupZxAB3j3tMoDJLZJG","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01CX8ioHCXA4LEW5yQoiVmtR","name":"Bash","input":{"command":"pwd && ls *.py","description":"Show current directory and list Python files"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":7,"cache_creation_input_tokens":116,"cache_read_input_tokens":7211,"cache_creation":{"ephemeral_5m_input_tokens":116,"ephemeral_1h_input_tokens":0},"output_tokens":25,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"6939fe31-d677-4c31-9307-4891924f760e","uuid":"3428c583-23b1-4941-a3f7-2efe2784b415"}
{"type":"user","message":{"role":"user","content":[{"type":"tool_result","content":"This Bash command contains multiple operations. The following part requires approval: ls *.py","is_error":true,"tool_use_id":"toolu_01CX8ioHCXA4LEW5yQoiVmtR"}]},"parent_tool_use_id":null,"session_id":"6939fe31-d677-4c31-9307-4891924f760e","uuid":"1b1ced93-080b-4533-b7f4-ec8e26fde84d","tool_use_result":"Error: This Bash command contains multiple operations. The following part requires approval: ls *.py"}
{"type":"assistant","message":{"model":"claude-opus-4-20250514","id":"msg_01DxdTErk6VYwmxQmZdXLbFY","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01MZMASkcRDkpedt3LkyF1RF","name":"Bash","input":{"command":"pwd","description":"Show current working directory"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":7,"cache_creation_input_tokens":118,"cache_read_input_tokens":7327,"cache_creation":{"ephemeral_5m_input_tokens":118,"ephemeral_1h_input_tokens":0},"output_tokens":25,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"6939fe31-d677-4c31-9307-4891924f760e","uuid":"395b6c0d-d21b-4403-8aa6-5d8aa6425717"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01MZMASkcRDkpedt3LkyF1RF","type":"tool_result","content":"/Users/emmayu/crqbench/projects/clusterfuzz/src/python/bot/tasks","is_error":false}]},"parent_tool_use_id":null,"session_id":"6939fe31-d677-4c31-9307-4891924f760e","uuid":"d100b4af-a931-424e-8066-8048c08e37fd","tool_use_result":{"stdout":"/Users/emmayu/crqbench/projects/clusterfuzz/src/python/bot/tasks","stderr":"","interrupted":false,"isImage":false}}
{"type":"assistant","message":{"model":"claude-opus-4-20250514","id":"msg_01MEze7WMX7pkz2CBFUSgpMM","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01BtjUe9kJbHM8QGnkRmcZ4g","name":"Bash","input":{"command":"find . -name \"*.py\" -type f","description":"Find all Python files in current directory"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":6,"cache_creation_input_tokens":109,"cache_read_input_tokens":7445,"cache_creation":{"ephemeral_5m_input_tokens":109,"ephemeral_1h_input_tokens":0},"output_tokens":24,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"6939fe31-d677-4c31-9307-4891924f760e","uuid":"1b460a5e-d4d8-487e-8f82-2507a84b397a"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01BtjUe9kJbHM8QGnkRmcZ4g","type":"tool_result","content":"./unpack_task.py\n./trials.py\n./minimize_task.py\n./corpus_pruning_task.py\n./ml_train_task.py\n./fuzz_task.py\n./symbolize_task.py\n./analyze_task.py\n./__init__.py\n./progression_task.py\n./upload_reports_task.py\n./task_creation.py\n./setup.py\n./blame_task.py\n./update_task.py\n./variant_task.py\n./regression_task.py\n./commands.py\n./impact_task.py","is_error":false}]},"parent_tool_use_id":null,"session_id":"6939fe31-d677-4c31-9307-4891924f760e","uuid":"5451cfa8-3677-4419-8a33-b77b7f71763a","tool_use_result":{"stdout":"./unpack_task.py\n./trials.py\n./minimize_task.py\n./corpus_pruning_task.py\n./ml_train_task.py\n./fuzz_task.py\n./symbolize_task.py\n./analyze_task.py\n./__init__.py\n./progression_task.py\n./upload_reports_task.py\n./task_creation.py\n./setup.py\n./blame_task.py\n./update_task.py\n./variant_task.py\n./regression_task.py\n./commands.py\n./impact_task.py","stderr":"","interrupted":false,"isImage":false}}
{"type":"assistant","message":{"model":"claude-opus-4-20250514","id":"msg_015VfGhDRKA7iV3o3WT1i1ht","type":"message","role":"assistant","content":[{"type":"text","text":"Since you're asking about a `if not build_directory` check, let me search for this pattern across these files:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":6,"cache_creation_input_tokens":232,"cache_read_input_tokens":7554,"cache_creation":{"ephemeral_5m_input_tokens":232,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"6939fe31-d677-4c31-9307-4891924f760e","uuid":"7016f39b-5603-479e-8380-c034b46e6e5c"}
{"type":"assistant","message":{"model":"claude-opus-4-20250514","id":"msg_015VfGhDRKA7iV3o3WT1i1ht","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01SjH5SvZeY4iRJ32KzLFvrg","name":"Bash","input":{"command":"grep -n \"if not build_directory\" *.py","description":"Search for 'if not build_directory' in Python files"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":6,"cache_creation_input_tokens":232,"cache_read_input_tokens":7554,"cache_creation":{"ephemeral_5m_input_tokens":232,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"6939fe31-d677-4c31-9307-4891924f760e","uuid":"465d8d97-12fd-4771-a184-4a910c6772dd"}
{"type":"user","message":{"role":"user","content":[{"type":"tool_result","content":"This command requires approval","is_error":true,"tool_use_id":"toolu_01SjH5SvZeY4iRJ32KzLFvrg"}]},"parent_tool_use_id":null,"session_id":"6939fe31-d677-4c31-9307-4891924f760e","uuid":"507bb9dc-a8ea-4b50-8ed0-a14eddc0c086","tool_use_result":"Error: This command requires approval"}
{"type":"assistant","message":{"model":"claude-opus-4-20250514","id":"msg_01HLb7669wya6EmeqjYVucyd","type":"message","role":"assistant","content":[{"type":"text","text":"Let me search for build_directory usage:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":7,"cache_creation_input_tokens":142,"cache_read_input_tokens":7786,"cache_creation":{"ephemeral_5m_input_tokens":142,"ephemeral_1h_input_tokens":0},"output_tokens":5,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"6939fe31-d677-4c31-9307-4891924f760e","uuid":"1fc7634c-1c6a-4f29-affd-f9711186a51a"}
{"type":"assistant","message":{"model":"claude-opus-4-20250514","id":"msg_01HLb7669wya6EmeqjYVucyd","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_018TcZKDiZdyZGHHfbRxLy7z","name":"Bash","input":{"command":"grep -n \"build_directory\" *.py | grep -v \"^Binary\"","description":"Search for build_directory references in Python files"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":7,"cache_creation_input_tokens":142,"cache_read_input_tokens":7786,"cache_creation":{"ephemeral_5m_input_tokens":142,"ephemeral_1h_input_tokens":0},"output_tokens":5,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"6939fe31-d677-4c31-9307-4891924f760e","uuid":"3fe16d9b-5327-4ad6-8b8b-2568c148f5cc"}
{"type":"user","message":{"role":"user","content":[{"type":"tool_result","content":"This Bash command contains multiple operations. The following part requires approval: grep -n \"build_directory\" *.py","is_error":true,"tool_use_id":"toolu_018TcZKDiZdyZGHHfbRxLy7z"}]},"parent_tool_use_id":null,"session_id":"6939fe31-d677-4c31-9307-4891924f760e","uuid":"41c718a1-f449-483b-abdd-771af1b1421f","tool_use_result":"Error: This Bash command contains multiple operations. The following part requires approval: grep -n \"build_directory\" *.py"}
{"type":"assistant","message":{"model":"claude-opus-4-20250514","id":"msg_01Sc9TUwJFh72tGfzEeFhLZv","type":"message","role":"assistant","content":[{"type":"text","text":"Let me check each file individually. Based on the recent commits mentioning corpus pruning, let me start with that file:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":7,"cache_creation_input_tokens":150,"cache_read_input_tokens":7928,"cache_creation":{"ephemeral_5m_input_tokens":150,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"6939fe31-d677-4c31-9307-4891924f760e","uuid":"ec138413-57f6-4d8a-ad17-a2a5bc0793f3"}
{"type":"assistant","message":{"model":"claude-opus-4-20250514","id":"msg_01Sc9TUwJFh72tGfzEeFhLZv","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_011x9519R14KyUjptUDzG9x1","name":"Read","input":{"file_path":"/Users/emmayu/crqbench/projects/clusterfuzz/src/python/bot/tasks/corpus_pruning_task.py"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":7,"cache_creation_input_tokens":150,"cache_read_input_tokens":7928,"cache_creation":{"ephemeral_5m_input_tokens":150,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"6939fe31-d677-4c31-9307-4891924f760e","uuid":"bb8f36c7-fbde-44a4-ae64-e912792978ef"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_011x9519R14KyUjptUDzG9x1","type":"tool_result","content":"     1→# Copyright 2019 Google LLC\n     2→#\n     3→# Licensed under the Apache License, Version 2.0 (the \"License\");\n     4→# you may not use this file except in compliance with the License.\n     5→# You may obtain a copy of the License at\n     6→#\n     7→#      http://www.apache.org/licenses/LICENSE-2.0\n     8→#\n     9→# Unless required by applicable law or agreed to in writing, software\n    10→# distributed under the License is distributed on an \"AS IS\" BASIS,\n    11→# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    12→# See the License for the specific language governing permissions and\n    13→# limitations under the License.\n    14→\"\"\"Corpus pruning task.\"\"\"\n    15→\n    16→from builtins import object\n    17→from builtins import zip\n    18→\n    19→import collections\n    20→import datetime\n    21→import os\n    22→import random\n    23→import shutil\n    24→\n    25→from google.cloud import ndb\n    26→\n    27→from base import utils\n    28→from bot.fuzzers import engine\n    29→from bot.fuzzers import engine_common\n    30→from bot.fuzzers import options\n    31→from bot.tasks import setup\n    32→from bot.tasks import task_creation\n    33→from build_management import build_manager\n    34→from crash_analysis import crash_analyzer\n    35→from crash_analysis.stack_parsing import stack_analyzer\n    36→from crash_analysis.stack_parsing import stack_symbolizer\n    37→from datastore import corpus_tagging\n    38→from datastore import data_handler\n    39→from datastore import data_types\n    40→from datastore import fuzz_target_utils\n    41→from enum import Enum\n    42→from fuzzing import corpus_manager\n    43→from fuzzing import leak_blacklist\n    44→from google_cloud_utils import big_query\n    45→from google_cloud_utils import blobs\n    46→from google_cloud_utils import storage\n    47→from metrics import logs\n    48→from system import archive\n    49→from system import environment\n    50→from system import shell\n    51→\n    52→# TODO(ochang): Move common libFuzzer code from fuzzer into CF.\n    53→\n    54→# Redzone size for running testcase.\n    55→DEFAULT_REDZONE = 32\n    56→\n    57→# Minimum redzone size for use during merging.\n    58→MIN_REDZONE = 16\n    59→\n    60→# Timeout for corpus pruning. Note that our priority is to make sure fuzzer's\n    61→# own corpus is minimized. If time remains, we spend time on using units from\n    62→# shared corpus. This combined timeout should be lower than task lease timeout.\n    63→CORPUS_PRUNING_TIMEOUT = 22 * 60 * 60\n    64→\n    65→# Time to allow libFuzzer to timeout on its own.\n    66→SINGLE_UNIT_TIMEOUT = 5\n    67→TIMEOUT_FLAG = '-timeout=%d' % SINGLE_UNIT_TIMEOUT\n    68→\n    69→# Corpus files limit for cases when corpus pruning task failed in the last\n    70→# execution.\n    71→CORPUS_FILES_LIMIT_FOR_FAILURES = 10000\n    72→\n    73→# Corpus total size limit for cases when corpus pruning task failed in the last\n    74→# execution.\n    75→CORPUS_SIZE_LIMIT_FOR_FAILURES = 2 * 1024 * 1024 * 1024  # 2 GB.\n    76→\n    77→# Maximum number of units to restore from quarantine in one run.\n    78→MAX_QUARANTINE_UNITS_TO_RESTORE = 128\n    79→\n    80→# Memory limits for testcase.\n    81→RSS_LIMIT = 2560\n    82→RSS_LIMIT_MB_FLAG = '-rss_limit_mb=%d'\n    83→\n    84→# Flag to disable leak checking.\n    85→DISABLE_LEAK_CHECK_FLAG = '-detect_leaks=0'\n    86→\n    87→# Flag to do value profile during merges.\n    88→USE_VALUE_PROFILE_FLAG = '-use_value_profile=%d'\n    89→\n    90→# Corpus size limit to allow use of value profile. This prevents corpus from\n    91→# growing unbounded.\n    92→CORPUS_SIZE_LIMIT_FOR_VALUE_PROFILE = 50000\n    93→\n    94→# Longer than default sync timeout to fix broken (overly large) corpora without\n    95→# losing coverage.\n    96→SYNC_TIMEOUT = 2 * 60 * 60\n    97→\n    98→# Number of fuzz targets whose backup corpus is used to cross pollinate with our\n    99→# current fuzz target corpus.\n   100→CROSS_POLLINATE_FUZZER_COUNT = 3\n   101→\n   102→CorpusPruningResult = collections.namedtuple('CorpusPruningResult', [\n   103→    'coverage_info', 'crashes', 'fuzzer_binary_name', 'revision',\n   104→    'cross_pollination_stats'\n   105→])\n   106→\n   107→CorpusCrash = collections.namedtuple('CorpusCrash', [\n   108→    'crash_state',\n   109→    'crash_type',\n   110→    'crash_address',\n   111→    'crash_stacktrace',\n   112→    'unit_path',\n   113→    'security_flag',\n   114→])\n   115→\n   116→CrossPollinationStats = collections.namedtuple('CrossPollinationStats', [\n   117→    'project_qualified_name', 'method', 'sources', 'tags',\n   118→    'initial_corpus_size', 'corpus_size', 'initial_edge_coverage',\n   119→    'edge_coverage', 'initial_feature_coverage', 'feature_coverage'\n   120→])\n   121→\n   122→\n   123→class Pollination(str, Enum):\n   124→  RANDOM = 'random'\n   125→  TAGGED = 'tagged'\n   126→\n   127→\n   128→def _get_corpus_file_paths(corpus_path):\n   129→  \"\"\"Return full paths to corpus files in |corpus_path|.\"\"\"\n   130→  return [\n   131→      os.path.join(corpus_path, filename)\n   132→      for filename in os.listdir(corpus_path)\n   133→  ]\n   134→\n   135→\n   136→def _limit_corpus_size(corpus_url):\n   137→  \"\"\"Limit number of files and size of a corpus.\"\"\"\n   138→  corpus_count = 0\n   139→  corpus_size = 0\n   140→  deleted_corpus_count = 0\n   141→  bucket, _ = storage.get_bucket_name_and_path(corpus_url)\n   142→  for corpus_file in storage.get_blobs(corpus_url):\n   143→    corpus_count += 1\n   144→    corpus_size += corpus_file['size']\n   145→    if (corpus_count > CORPUS_FILES_LIMIT_FOR_FAILURES or\n   146→        corpus_size > CORPUS_SIZE_LIMIT_FOR_FAILURES):\n   147→      path_to_delete = storage.get_cloud_storage_file_path(\n   148→          bucket, corpus_file['name'])\n   149→      storage.delete(path_to_delete)\n   150→      deleted_corpus_count += 1\n   151→\n   152→  if deleted_corpus_count:\n   153→    logs.log('Removed %d files from oversized corpus: %s.' %\n   154→             (deleted_corpus_count, corpus_url))\n   155→\n   156→\n   157→def _get_time_remaining(start_time):\n   158→  \"\"\"Return time remaining.\"\"\"\n   159→  time_used = int((datetime.datetime.utcnow() - start_time).total_seconds())\n   160→  return CORPUS_PRUNING_TIMEOUT - time_used\n   161→\n   162→\n   163→class CorpusPruningException(Exception):\n   164→  \"\"\"Corpus pruning exception.\"\"\"\n   165→\n   166→\n   167→class CrossPollinateFuzzer(object):\n   168→  \"\"\"Cross Pollinate Fuzzer.\"\"\"\n   169→\n   170→  def __init__(self, fuzz_target, backup_bucket_name, corpus_engine_name):\n   171→    self.fuzz_target = fuzz_target\n   172→    self.backup_bucket_name = backup_bucket_name\n   173→    self.corpus_engine_name = corpus_engine_name\n   174→\n   175→\n   176→class Context(object):\n   177→  \"\"\"Pruning state.\"\"\"\n   178→\n   179→  def __init__(self,\n   180→               fuzz_target,\n   181→               cross_pollinate_fuzzers,\n   182→               cross_pollination_method=Pollination.RANDOM,\n   183→               tag=None):\n   184→\n   185→    self.fuzz_target = fuzz_target\n   186→    self.cross_pollinate_fuzzers = cross_pollinate_fuzzers\n   187→    self.cross_pollination_method = cross_pollination_method\n   188→    self.tag = tag\n   189→\n   190→    self.merge_tmp_dir = None\n   191→    self.engine = engine.get(self.fuzz_target.engine)\n   192→    if not self.engine:\n   193→      raise CorpusPruningException('Engine {} not found'.format(engine))\n   194→\n   195→    self._created_directories = []\n   196→\n   197→    # Set up temporary directories where corpora will be synced to.\n   198→    # Initial synced corpus.\n   199→    self.initial_corpus_path = self._create_temp_corpus_directory(\n   200→        '%s_initial_corpus' % self.fuzz_target.project_qualified_name())\n   201→    # Minimized corpus.\n   202→    self.minimized_corpus_path = self._create_temp_corpus_directory(\n   203→        '%s_minimized_corpus' % self.fuzz_target.project_qualified_name())\n   204→    # Synced quarantine corpus.\n   205→    self.quarantine_corpus_path = self._create_temp_corpus_directory(\n   206→        '%s_quarantine' % self.fuzz_target.project_qualified_name())\n   207→    # Synced shared corpus.\n   208→    self.shared_corpus_path = self._create_temp_corpus_directory(\n   209→        '%s_shared' % self.fuzz_target.project_qualified_name())\n   210→    # Bad units.\n   211→    self.bad_units_path = self._create_temp_corpus_directory(\n   212→        '%s_bad_units' % self.fuzz_target.project_qualified_name())\n   213→    self.merge_tmp_dir = self._create_temp_corpus_directory('merge_workdir')\n   214→\n   215→    self.corpus = corpus_manager.FuzzTargetCorpus(\n   216→        self.fuzz_target.engine, self.fuzz_target.project_qualified_name())\n   217→    self.quarantine_corpus = corpus_manager.FuzzTargetCorpus(\n   218→        self.fuzz_target.engine,\n   219→        self.fuzz_target.project_qualified_name(),\n   220→        quarantine=True)\n   221→\n   222→    shared_corpus_bucket = environment.get_value('SHARED_CORPUS_BUCKET')\n   223→    self.shared_corpus = corpus_manager.GcsCorpus(shared_corpus_bucket)\n   224→\n   225→  def restore_quarantined_units(self):\n   226→    \"\"\"Restore units from the quarantine.\"\"\"\n   227→    logs.log('Restoring units from quarantine.')\n   228→    # Limit the number of quarantine units to restore, in case there are a lot.\n   229→    quarantine_unit_paths = _get_corpus_file_paths(self.quarantine_corpus_path)\n   230→    if len(quarantine_unit_paths) > MAX_QUARANTINE_UNITS_TO_RESTORE:\n   231→      quarantine_unit_paths = random.sample(quarantine_unit_paths,\n   232→                                            MAX_QUARANTINE_UNITS_TO_RESTORE)\n   233→\n   234→    for unit_path in quarantine_unit_paths:\n   235→      unit_filename = os.path.basename(unit_path)\n   236→      shutil.move(unit_path,\n   237→                  os.path.join(self.initial_corpus_path, unit_filename))\n   238→\n   239→  def _create_temp_corpus_directory(self, name):\n   240→    \"\"\"Create temporary corpus directory. Returns path to the created\n   241→    directory.\"\"\"\n   242→    testcases_directory = environment.get_value('FUZZ_INPUTS_DISK')\n   243→    directory_path = os.path.join(testcases_directory, name)\n   244→    shell.create_directory(directory_path)\n   245→    self._created_directories.append(directory_path)\n   246→\n   247→    return directory_path\n   248→\n   249→  def sync_to_disk(self):\n   250→    \"\"\"Sync required corpora to disk.\"\"\"\n   251→    if not self.corpus.rsync_to_disk(\n   252→        self.initial_corpus_path, timeout=SYNC_TIMEOUT):\n   253→      raise CorpusPruningException('Failed to sync corpus to disk.')\n   254→\n   255→    if not self.quarantine_corpus.rsync_to_disk(self.quarantine_corpus_path):\n   256→      logs.log_error(\n   257→          'Failed to sync quarantine corpus to disk.',\n   258→          fuzz_target=self.fuzz_target)\n   259→\n   260→    if not self.shared_corpus.rsync_to_disk(self.shared_corpus_path):\n   261→      logs.log_error(\n   262→          'Failed to sync shared corpus to disk.', fuzz_target=self.fuzz_target)\n   263→\n   264→    self._cross_pollinate_other_fuzzer_corpuses()\n   265→\n   266→  def sync_to_gcs(self):\n   267→    \"\"\"Sync corpora to GCS post merge.\"\"\"\n   268→    if not self.corpus.rsync_from_disk(self.minimized_corpus_path):\n   269→      raise CorpusPruningException('Failed to sync minimized corpus to gcs.')\n   270→\n   271→  def cleanup(self):\n   272→    \"\"\"Cleanup state.\"\"\"\n   273→    for path in self._created_directories:\n   274→      shell.remove_directory(path)\n   275→\n   276→  def _cross_pollinate_other_fuzzer_corpuses(self):\n   277→    \"\"\"Add other fuzzer corpuses to shared corpus path for cross-pollination.\"\"\"\n   278→    corpus_backup_date = utils.utcnow().date() - datetime.timedelta(\n   279→        days=data_types.CORPUS_BACKUP_PUBLIC_LOOKBACK_DAYS)\n   280→\n   281→    for cross_pollinate_fuzzer in self.cross_pollinate_fuzzers:\n   282→      project_qualified_name = (\n   283→          cross_pollinate_fuzzer.fuzz_target.project_qualified_name())\n   284→      backup_bucket_name = cross_pollinate_fuzzer.backup_bucket_name\n   285→      corpus_engine_name = cross_pollinate_fuzzer.corpus_engine_name\n   286→\n   287→      corpus_backup_url = corpus_manager.gcs_url_for_backup_file(\n   288→          backup_bucket_name, corpus_engine_name, project_qualified_name,\n   289→          corpus_backup_date)\n   290→      corpus_backup_local_filename = '%s-%s' % (\n   291→          project_qualified_name, os.path.basename(corpus_backup_url))\n   292→      corpus_backup_local_path = os.path.join(self.shared_corpus_path,\n   293→                                              corpus_backup_local_filename)\n   294→\n   295→      if not storage.exists(corpus_backup_url, ignore_errors=True):\n   296→        # This can happen in cases when a new fuzz target is checked in or if\n   297→        # missed to capture a backup for a particular day (for OSS-Fuzz, this\n   298→        # will result in a 403 instead of 404 since that GCS path belongs to\n   299→        # other project). So, just log a warning for debugging purposes only.\n   300→        logs.log_warn(\n   301→            'Corpus backup does not exist, ignoring: %s.' % corpus_backup_url)\n   302→        continue\n   303→\n   304→      if not storage.copy_file_from(corpus_backup_url,\n   305→                                    corpus_backup_local_path):\n   306→        continue\n   307→\n   308→      corpus_backup_output_directory = os.path.join(self.shared_corpus_path,\n   309→                                                    project_qualified_name)\n   310→      shell.create_directory(corpus_backup_output_directory)\n   311→      result = archive.unpack(corpus_backup_local_path,\n   312→                              corpus_backup_output_directory)\n   313→      shell.remove_file(corpus_backup_local_path)\n   314→\n   315→      if result:\n   316→        logs.log(\n   317→            'Corpus backup url %s successfully unpacked into shared corpus.' %\n   318→            corpus_backup_url)\n   319→      else:\n   320→        logs.log_error(\n   321→            'Failed to unpack corpus backup from url %s.' % corpus_backup_url)\n   322→\n   323→\n   324→class Runner(object):\n   325→  \"\"\"Runner for libFuzzer.\"\"\"\n   326→\n   327→  def __init__(self, build_directory, context):\n   328→    self.build_directory = build_directory\n   329→    self.context = context\n   330→\n   331→    self.target_path = engine_common.find_fuzzer_path(\n   332→        self.build_directory, self.context.fuzz_target.binary)\n   333→    if not self.target_path:\n   334→      raise CorpusPruningException(\n   335→          'Failed to get fuzzer path for %s.' % self.context.fuzz_target.binary)\n   336→\n   337→    self.fuzzer_options = options.get_fuzz_target_options(self.target_path)\n   338→\n   339→  def get_libfuzzer_flags(self):\n   340→    \"\"\"Get default libFuzzer options.\"\"\"\n   341→    if self.fuzzer_options:\n   342→      libfuzzer_arguments = self.fuzzer_options.get_engine_arguments(\n   343→          'libfuzzer')\n   344→\n   345→      # Allow some flags to be used from .options file for single unit testing.\n   346→      # Allow specifying a lower rss_limit.\n   347→      rss_limit = libfuzzer_arguments.get('rss_limit_mb', constructor=int)\n   348→      if not rss_limit or rss_limit > RSS_LIMIT:\n   349→        rss_limit = RSS_LIMIT\n   350→\n   351→      # Some targets might falsely report leaks all the time, so allow this to\n   352→      # be disabled.\n   353→      detect_leaks = libfuzzer_arguments.get('detect_leaks', default='1')\n   354→      arguments = [\n   355→          RSS_LIMIT_MB_FLAG % rss_limit,\n   356→          '-detect_leaks=%s' % detect_leaks, TIMEOUT_FLAG\n   357→      ]\n   358→    else:\n   359→      arguments = [RSS_LIMIT_MB_FLAG % RSS_LIMIT, TIMEOUT_FLAG]\n   360→\n   361→    corpus_size = shell.get_directory_file_count(\n   362→        self.context.initial_corpus_path)\n   363→    use_value_profile = int(corpus_size <= CORPUS_SIZE_LIMIT_FOR_VALUE_PROFILE)\n   364→    arguments.append(USE_VALUE_PROFILE_FLAG % use_value_profile)\n   365→\n   366→    return arguments\n   367→\n   368→  def process_sanitizer_options(self):\n   369→    \"\"\"Process sanitizer options overrides.\"\"\"\n   370→    if not self.fuzzer_options:\n   371→      return\n   372→\n   373→    # Only need to look as ASan, as that's what we prune with.\n   374→    overrides = self.fuzzer_options.get_asan_options()\n   375→    if not overrides:\n   376→      return\n   377→\n   378→    asan_options = environment.get_memory_tool_options('ASAN_OPTIONS')\n   379→    asan_options.update(overrides)\n   380→    environment.set_memory_tool_options('ASAN_OPTIONS', asan_options)\n   381→\n   382→  def reproduce(self, input_path, arguments, max_time):\n   383→    return self.context.engine.reproduce(self.target_path, input_path,\n   384→                                         arguments, max_time)\n   385→\n   386→  def minimize_corpus(self, arguments, input_dirs, output_dir, reproducers_dir,\n   387→                      max_time):\n   388→    return self.context.engine.minimize_corpus(self.target_path, arguments,\n   389→                                               input_dirs, output_dir,\n   390→                                               reproducers_dir, max_time)\n   391→\n   392→\n   393→class CorpusPruner(object):\n   394→  \"\"\"Class that handles corpus pruning.\"\"\"\n   395→\n   396→  def __init__(self, runner):\n   397→    self.runner = runner\n   398→    self.context = self.runner.context\n   399→\n   400→  def _run_single_unit(self, unit_path):\n   401→    \"\"\"Run a single unit, and return the result.\"\"\"\n   402→    arguments = self.runner.get_libfuzzer_flags()\n   403→    return self.runner.reproduce(unit_path, arguments, SINGLE_UNIT_TIMEOUT)\n   404→\n   405→  def _quarantine_unit(self, unit_path, quarantine_corpus_path):\n   406→    \"\"\"Moves the given unit to the quarantine, and returns the path to the unit\n   407→    in the quarantine.\"\"\"\n   408→    quarantined_unit_path = os.path.join(quarantine_corpus_path,\n   409→                                         os.path.basename(unit_path))\n   410→    shutil.move(unit_path, quarantined_unit_path)\n   411→\n   412→    return quarantined_unit_path\n   413→\n   414→  def process_bad_units(self, bad_units_path, quarantine_corpus_path, crashes):\n   415→    \"\"\"Process bad units found during merge.\"\"\"\n   416→    # TODO(ochang): A lot of this function is similar to parts of fuzz_task.\n   417→    # Ideally fuzz_task can be refactored in a way that lets us share the common\n   418→    # code.\n   419→\n   420→    environment.reset_current_memory_tool_options(redzone_size=DEFAULT_REDZONE)\n   421→    self.runner.process_sanitizer_options()\n   422→\n   423→    logs.log('Processing bad units.')\n   424→    corpus_file_paths = _get_corpus_file_paths(bad_units_path)\n   425→    num_bad_units = 0\n   426→\n   427→    # Run each corpus item individually.\n   428→    for i, unit_path in enumerate(corpus_file_paths, 1):\n   429→      if i % 100 == 0:\n   430→        logs.log('Up to %d' % i)\n   431→\n   432→      unit_name = os.path.basename(unit_path)\n   433→      if unit_name.startswith('timeout-') or unit_name.startswith('oom-'):\n   434→        # Don't waste time re-running timeout or oom testcases.\n   435→        unit_path = self._quarantine_unit(unit_path, quarantine_corpus_path)\n   436→        num_bad_units += 1\n   437→        continue\n   438→\n   439→      try:\n   440→        result = self._run_single_unit(unit_path)\n   441→      except engine.TimeoutError:\n   442→        # Slow unit. Quarantine it.\n   443→        unit_path = self._quarantine_unit(unit_path, quarantine_corpus_path)\n   444→        num_bad_units += 1\n   445→        continue\n   446→\n   447→      if not crash_analyzer.is_memory_tool_crash(result.output):\n   448→        # Didn't crash.\n   449→        continue\n   450→\n   451→      # Get memory tool crash information.\n   452→      state = stack_analyzer.get_crash_data(result.output, symbolize_flag=True)\n   453→\n   454→      # Crashed or caused a leak. Quarantine it.\n   455→      unit_path = self._quarantine_unit(unit_path, quarantine_corpus_path)\n   456→      num_bad_units += 1\n   457→\n   458→      if crash_analyzer.ignore_stacktrace(state.crash_stacktrace):\n   459→        continue\n   460→\n   461→      # Local de-duplication.\n   462→      if state.crash_state not in crashes:\n   463→        security_flag = crash_analyzer.is_security_issue(\n   464→            state.crash_stacktrace, state.crash_type, state.crash_address)\n   465→        crashes[state.crash_state] = CorpusCrash(\n   466→            state.crash_state, state.crash_type, state.crash_address,\n   467→            state.crash_stacktrace, unit_path, security_flag)\n   468→\n   469→    logs.log('Found %d bad units, %d unique crashes.' % (num_bad_units,\n   470→                                                         len(crashes)))\n   471→\n   472→  def run(self, initial_corpus_path, minimized_corpus_path, bad_units_path):\n   473→    \"\"\"Run corpus pruning. Output result to directory.\"\"\"\n   474→    if not shell.get_directory_file_count(initial_corpus_path):\n   475→      # Empty corpus, nothing to do.\n   476→      return None\n   477→\n   478→    # Set memory tool options and fuzzer arguments.\n   479→    engine_common.unpack_seed_corpus_if_needed(\n   480→        self.runner.target_path, initial_corpus_path, force_unpack=True)\n   481→\n   482→    environment.reset_current_memory_tool_options(\n   483→        redzone_size=MIN_REDZONE, leaks=True)\n   484→    self.runner.process_sanitizer_options()\n   485→    additional_args = self.runner.get_libfuzzer_flags()\n   486→\n   487→    # Execute fuzzer with arguments for corpus pruning.\n   488→    logs.log('Running merge...')\n   489→    try:\n   490→      result = self.runner.minimize_corpus(\n   491→          additional_args, [initial_corpus_path], minimized_corpus_path,\n   492→          bad_units_path, CORPUS_PRUNING_TIMEOUT)\n   493→    except engine.TimeoutError as e:\n   494→      raise CorpusPruningException(\n   495→          'Corpus pruning timed out while minimizing corpus\\n' + repr(e))\n   496→    except engine.Error as e:\n   497→      raise CorpusPruningException(\n   498→          'Corpus pruning failed to minimize corpus\\n' + repr(e))\n   499→\n   500→    symbolized_output = stack_symbolizer.symbolize_stacktrace(result.logs)\n   501→\n   502→    # Sanity check that there are files in minimized corpus after merging.\n   503→    if not shell.get_directory_file_count(minimized_corpus_path):\n   504→      raise CorpusPruningException(\n   505→          'Corpus pruning failed to minimize corpus\\n' + symbolized_output)\n   506→\n   507→    logs.log('Corpus merge finished successfully.', output=symbolized_output)\n   508→\n   509→    return result.stats\n   510→\n   511→\n   512→class CrossPollinator(object):\n   513→  \"\"\"Cross pollination.\"\"\"\n   514→\n   515→  def __init__(self, runner):\n   516→    self.runner = runner\n   517→    self.context = self.runner.context\n   518→\n   519→  def run(self, timeout):\n   520→    \"\"\"Merge testcases from corpus from other fuzz targets.\"\"\"\n   521→    if not shell.get_directory_file_count(self.context.shared_corpus_path):\n   522→      logs.log('No files found in shared corpus, skip merge.')\n   523→      return None\n   524→\n   525→    # Run pruning on the shared corpus and log the result in case of error.\n   526→    logs.log('Merging shared corpus...')\n   527→    environment.reset_current_memory_tool_options(redzone_size=DEFAULT_REDZONE)\n   528→    self.runner.process_sanitizer_options()\n   529→\n   530→    additional_args = self.runner.get_libfuzzer_flags()\n   531→\n   532→    try:\n   533→      result = self.runner.minimize_corpus(additional_args,\n   534→                                           [self.context.shared_corpus_path],\n   535→                                           self.context.minimized_corpus_path,\n   536→                                           self.context.bad_units_path, timeout)\n   537→      symbolized_output = stack_symbolizer.symbolize_stacktrace(result.logs)\n   538→      logs.log(\n   539→          'Shared corpus merge finished successfully.',\n   540→          output=symbolized_output)\n   541→    except engine.TimeoutError as e:\n   542→      # Other cross pollinated fuzzer corpuses can have unexpected test cases\n   543→      # that time us out. This is expected, so bail out.\n   544→      logs.log_warn('Corpus pruning timed out while merging shared corpus\\n' +\n   545→                    repr(e))\n   546→      return None\n   547→    except engine.Error as e:\n   548→      # Other cross pollinated fuzzer corpuses can be large, so we can run out\n   549→      # of disk space and exception out. This is expected, so bail out.\n   550→      logs.log_warn('Corpus pruning failed to merge shared corpus\\n' + repr(e))\n   551→      return None\n   552→\n   553→    return result.stats\n   554→\n   555→\n   556→def _record_cross_pollination_stats(stats):\n   557→  \"\"\"Log stats about cross pollination in BigQuery.\"\"\"\n   558→  # If no stats were gathered due to a timeout or lack of corpus, return.\n   559→  if not stats:\n   560→    return\n   561→\n   562→  bigquery_row = {\n   563→      'project_qualified_name': stats.project_qualified_name,\n   564→      'method': stats.method,\n   565→      'sources': stats.sources,\n   566→      'tags': stats.tags if stats.tags else '',\n   567→      'initial_corpus_size': stats.initial_corpus_size,\n   568→      'corpus_size': stats.corpus_size,\n   569→      'initial_edge_coverage': stats.initial_edge_coverage,\n   570→      'edge_coverage': stats.edge_coverage,\n   571→      'initial_feature_coverage': stats.initial_feature_coverage,\n   572→      'feature_coverage': stats.feature_coverage\n   573→  }\n   574→\n   575→  # BigQuery not available in local development. This is necessary because the\n   576→  # untrusted runner is in a separate process and can't be easily mocked.\n   577→  # Check here instead of earlier to test as much of the function as we can.\n   578→  if environment.get_value('LOCAL_DEVELOPMENT') or environment.get_value(\n   579→      'PY_UNITTESTS'):\n   580→    return\n   581→\n   582→  client = big_query.Client(\n   583→      dataset_id='main', table_id='cross_pollination_statistics')\n   584→  client.insert([big_query.Insert(row=bigquery_row, insert_id=None)])\n   585→\n   586→\n   587→def do_corpus_pruning(context, last_execution_failed, revision):\n   588→  \"\"\"Run corpus pruning.\"\"\"\n   589→  # Set |FUZZ_TARGET| environment variable to help with unarchiving only fuzz\n   590→  # target and its related files.\n   591→  environment.set_value('FUZZ_TARGET', context.fuzz_target.binary)\n   592→\n   593→  if environment.is_trusted_host():\n   594→    from bot.untrusted_runner import tasks_host\n   595→    return tasks_host.do_corpus_pruning(context, last_execution_failed,\n   596→                                        revision)\n   597→\n   598→  build_setup_result = build_manager.setup_build(revision=revision)\n   599→  build_directory = environment.get_value('BUILD_DIR')\n   600→  if not build_setup_result or not build_directory:\n   601→    raise CorpusPruningException('Failed to setup build.')\n   602→\n   603→  start_time = datetime.datetime.utcnow()\n   604→  runner = Runner(build_directory, context)\n   605→  pruner = CorpusPruner(runner)\n   606→  fuzzer_binary_name = os.path.basename(runner.target_path)\n   607→\n   608→  # If our last execution failed, shrink to a randomized corpus of usable size\n   609→  # to prevent corpus from growing unbounded and recurring failures when trying\n   610→  # to minimize it.\n   611→  if last_execution_failed:\n   612→    for corpus_url in [\n   613→        context.corpus.get_gcs_url(),\n   614→        context.quarantine_corpus.get_gcs_url()\n   615→    ]:\n   616→      _limit_corpus_size(corpus_url)\n   617→\n   618→  # Get initial corpus to process from GCS.\n   619→  context.sync_to_disk()\n   620→  initial_corpus_size = shell.get_directory_file_count(\n   621→      context.initial_corpus_path)\n   622→\n   623→  # Restore a small batch of quarantined units back to corpus.\n   624→  context.restore_quarantined_units()\n   625→\n   626→  # Shrink to a minimized corpus using corpus merge.\n   627→  pruner_stats = pruner.run(context.initial_corpus_path,\n   628→                            context.minimized_corpus_path,\n   629→                            context.bad_units_path)\n   630→\n   631→  # Sync minimized corpus back to GCS.\n   632→  context.sync_to_gcs()\n   633→\n   634→  # Create corpus backup.\n   635→  backup_bucket = environment.get_value('BACKUP_BUCKET')\n   636→  corpus_backup_url = corpus_manager.backup_corpus(\n   637→      backup_bucket, context.corpus, context.minimized_corpus_path)\n   638→\n   639→  minimized_corpus_size_units = shell.get_directory_file_count(\n   640→      context.minimized_corpus_path)\n   641→  minimized_corpus_size_bytes = shell.get_directory_size(\n   642→      context.minimized_corpus_path)\n   643→\n   644→  logs.log('Corpus pruned from %d to %d units.' % (initial_corpus_size,\n   645→                                                   minimized_corpus_size_units))\n   646→\n   647→  # Process bad units found during merge.\n   648→  # Mapping of crash state -> CorpusCrash\n   649→  crashes = {}\n   650→  pruner.process_bad_units(context.bad_units_path,\n   651→                           context.quarantine_corpus_path, crashes)\n   652→  context.quarantine_corpus.rsync_from_disk(context.quarantine_corpus_path)\n   653→\n   654→  # Store corpus stats into CoverageInformation entity.\n   655→  project_qualified_name = context.fuzz_target.project_qualified_name()\n   656→  today = datetime.datetime.utcnow().date()\n   657→  coverage_info = data_types.CoverageInformation(\n   658→      fuzzer=project_qualified_name, date=today)\n   659→\n   660→  quarantine_corpus_size = shell.get_directory_file_count(\n   661→      context.quarantine_corpus_path)\n   662→  quarantine_corpus_dir_size = shell.get_directory_size(\n   663→      context.quarantine_corpus_path)\n   664→\n   665→  # Save the minimize corpus size before cross pollination to put in BigQuery.\n   666→  pre_pollination_corpus_size = minimized_corpus_size_units\n   667→\n   668→  # Populate coverage stats.\n   669→  coverage_info.corpus_size_units = minimized_corpus_size_units\n   670→  coverage_info.corpus_size_bytes = minimized_corpus_size_bytes\n   671→  coverage_info.quarantine_size_units = quarantine_corpus_size\n   672→  coverage_info.quarantine_size_bytes = quarantine_corpus_dir_size\n   673→  coverage_info.corpus_backup_location = corpus_backup_url\n   674→  coverage_info.corpus_location = context.corpus.get_gcs_url()\n   675→  coverage_info.quarantine_location = context.quarantine_corpus.get_gcs_url()\n   676→\n   677→  # Calculate remaining time to use for shared corpus merging.\n   678→  time_remaining = _get_time_remaining(start_time)\n   679→  if time_remaining <= 0:\n   680→    logs.log_warn('Not enough time for shared corpus merging.')\n   681→    return None\n   682→\n   683→  cross_pollinator = CrossPollinator(runner)\n   684→  pollinator_stats = cross_pollinator.run(time_remaining)\n   685→\n   686→  context.sync_to_gcs()\n   687→\n   688→  # Update corpus size stats.\n   689→  minimized_corpus_size_units = shell.get_directory_file_count(\n   690→      context.minimized_corpus_path)\n   691→  minimized_corpus_size_bytes = shell.get_directory_size(\n   692→      context.minimized_corpus_path)\n   693→  coverage_info.corpus_size_units = minimized_corpus_size_units\n   694→  coverage_info.corpus_size_bytes = minimized_corpus_size_bytes\n   695→\n   696→  logs.log('Finished.')\n   697→\n   698→  sources = ','.join([\n   699→      fuzzer.fuzz_target.project_qualified_name()\n   700→      for fuzzer in context.cross_pollinate_fuzzers\n   701→  ])\n   702→\n   703→  cross_pollination_stats = None\n   704→  if pruner_stats and pollinator_stats:\n   705→    cross_pollination_stats = CrossPollinationStats(\n   706→        project_qualified_name, context.cross_pollination_method, sources,\n   707→        context.tag, initial_corpus_size, pre_pollination_corpus_size,\n   708→        pruner_stats['edge_coverage'], pollinator_stats['edge_coverage'],\n   709→        pruner_stats['feature_coverage'], pollinator_stats['feature_coverage'])\n   710→\n   711→  return CorpusPruningResult(\n   712→      coverage_info=coverage_info,\n   713→      crashes=list(crashes.values()),\n   714→      fuzzer_binary_name=fuzzer_binary_name,\n   715→      revision=environment.get_value('APP_REVISION'),\n   716→      cross_pollination_stats=cross_pollination_stats)\n   717→\n   718→\n   719→def _process_corpus_crashes(context, result):\n   720→  \"\"\"Process crashes found in the corpus.\"\"\"\n   721→  # Default Testcase entity values.\n   722→  crash_revision = result.revision\n   723→  job_type = environment.get_value('JOB_NAME')\n   724→  minimized_arguments = '%TESTCASE% ' + context.fuzz_target.binary\n   725→  project_name = data_handler.get_project_name(job_type)\n   726→\n   727→  comment = 'Fuzzer %s generated corpus testcase crashed (r%s)' % (\n   728→      context.fuzz_target.project_qualified_name(), crash_revision)\n   729→\n   730→  # Generate crash reports.\n   731→  for crash in result.crashes:\n   732→    existing_testcase = data_handler.find_testcase(\n   733→        project_name, crash.crash_type, crash.crash_state, crash.security_flag)\n   734→    if existing_testcase:\n   735→      continue\n   736→\n   737→    # Upload/store testcase.\n   738→    if environment.is_trusted_host():\n   739→      from bot.untrusted_runner import file_host\n   740→      unit_path = os.path.join(context.bad_units_path,\n   741→                               os.path.basename(crash.unit_path))\n   742→      # Prevent the worker from escaping out of |context.bad_units_path|.\n   743→      if not file_host.is_directory_parent(unit_path, context.bad_units_path):\n   744→        raise CorpusPruningException('Invalid units path from worker.')\n   745→\n   746→      file_host.copy_file_from_worker(crash.unit_path, unit_path)\n   747→    else:\n   748→      unit_path = crash.unit_path\n   749→\n   750→    with open(unit_path, 'rb') as f:\n   751→      key = blobs.write_blob(f)\n   752→\n   753→    # Set the absolute_path property of the Testcase to a file in FUZZ_INPUTS\n   754→    # instead of the local quarantine directory.\n   755→    absolute_testcase_path = os.path.join(\n   756→        environment.get_value('FUZZ_INPUTS'), 'testcase')\n   757→\n   758→    testcase_id = data_handler.store_testcase(\n   759→        crash=crash,\n   760→        fuzzed_keys=key,\n   761→        minimized_keys='',\n   762→        regression='',\n   763→        fixed='',\n   764→        one_time_crasher_flag=False,\n   765→        crash_revision=crash_revision,\n   766→        comment=comment,\n   767→        absolute_path=absolute_testcase_path,\n   768→        fuzzer_name=context.fuzz_target.engine,\n   769→        fully_qualified_fuzzer_name=context.fuzz_target.fully_qualified_name(),\n   770→        job_type=job_type,\n   771→        archived=False,\n   772→        archive_filename='',\n   773→        binary_flag=True,\n   774→        http_flag=False,\n   775→        gestures=None,\n   776→        redzone=DEFAULT_REDZONE,\n   777→        disable_ubsan=False,\n   778→        minidump_keys=None,\n   779→        window_argument=None,\n   780→        timeout_multiplier=1.0,\n   781→        minimized_arguments=minimized_arguments)\n   782→\n   783→    # Set fuzzer_binary_name in testcase metadata.\n   784→    testcase = data_handler.get_testcase_by_id(testcase_id)\n   785→    testcase.set_metadata('fuzzer_binary_name', result.fuzzer_binary_name)\n   786→\n   787→    issue_metadata = engine_common.get_all_issue_metadata_for_testcase(testcase)\n   788→    if issue_metadata:\n   789→      for key, value in issue_metadata.items():\n   790→        testcase.set_metadata(key, value, update_testcase=False)\n   791→\n   792→      testcase.put()\n   793→\n   794→    # Create additional tasks for testcase (starting with minimization).\n   795→    testcase = data_handler.get_testcase_by_id(testcase_id)\n   796→    task_creation.create_tasks(testcase)\n   797→\n   798→\n   799→def _select_targets_and_jobs_for_pollination(engine_name, current_fuzzer_name,\n   800→                                             method, tag):\n   801→  \"\"\"Select jobs to use for cross pollination.\"\"\"\n   802→  target_jobs = list(fuzz_target_utils.get_fuzz_target_jobs(engine=engine_name))\n   803→\n   804→  if method == Pollination.TAGGED:\n   805→    similar_tagged_targets = [\n   806→        target.fully_qualified_fuzz_target_name\n   807→        for target in corpus_tagging.get_targets_with_tag(tag)\n   808→        if target.fully_qualified_fuzz_target_name != current_fuzzer_name\n   809→    ]\n   810→    # Intersect target_jobs and similar_tagged_targets on fully qualified\n   811→    # fuzz target name.\n   812→    target_jobs = [\n   813→        target for target in target_jobs\n   814→        if target.fuzz_target_name in similar_tagged_targets\n   815→    ]\n   816→\n   817→  targets = fuzz_target_utils.get_fuzz_targets_for_target_jobs(target_jobs)\n   818→\n   819→  targets_and_jobs = [(target, target_job)\n   820→                      for target, target_job in zip(targets, target_jobs)\n   821→                      if target_job.fuzz_target_name != current_fuzzer_name]\n   822→  selected_targets_and_jobs = random.SystemRandom().sample(\n   823→      targets_and_jobs, min(\n   824→          len(targets_and_jobs), CROSS_POLLINATE_FUZZER_COUNT))\n   825→\n   826→  return selected_targets_and_jobs\n   827→\n   828→\n   829→def _get_cross_pollinate_fuzzers(engine_name, current_fuzzer_name, method, tag):\n   830→  \"\"\"Return a list of fuzzer objects to use for cross pollination.\"\"\"\n   831→  cross_pollinate_fuzzers = []\n   832→\n   833→  selected_targets_and_jobs = _select_targets_and_jobs_for_pollination(\n   834→      engine_name, current_fuzzer_name, method, tag)\n   835→\n   836→  default_backup_bucket = utils.default_backup_bucket()\n   837→  for target, target_job in selected_targets_and_jobs:\n   838→    job = data_types.Job.query(data_types.Job.name == target_job.job).get()\n   839→    if not job:\n   840→      continue\n   841→\n   842→    job_environment = job.get_environment()\n   843→    backup_bucket_name = job_environment.get('BACKUP_BUCKET',\n   844→                                             default_backup_bucket)\n   845→    if not backup_bucket_name:\n   846→      continue\n   847→    corpus_engine_name = job_environment.get('CORPUS_FUZZER_NAME_OVERRIDE',\n   848→                                             engine_name)\n   849→\n   850→    cross_pollinate_fuzzers.append(\n   851→        CrossPollinateFuzzer(\n   852→            target,\n   853→            backup_bucket_name,\n   854→            corpus_engine_name,\n   855→        ))\n   856→\n   857→  return cross_pollinate_fuzzers\n   858→\n   859→\n   860→def _save_coverage_information(context, result):\n   861→  \"\"\"Saves coverage information in datastore using an atomic transaction.\"\"\"\n   862→\n   863→  # Use ndb.transaction with retries below to mitigate risk of a race condition.\n   864→  def _try_save_coverage_information():\n   865→    \"\"\"Implements save_coverage_information function.\"\"\"\n   866→    coverage_info = data_handler.get_coverage_information(\n   867→        context.fuzz_target.project_qualified_name(),\n   868→        result.coverage_info.date,\n   869→        create_if_needed=True)\n   870→\n   871→    # Intentionally skip edge and function coverage values as those would come\n   872→    # from fuzzer coverage cron task (see src/go/server/cron/coverage.go).\n   873→    coverage_info.corpus_size_units = result.coverage_info.corpus_size_units\n   874→    coverage_info.corpus_size_bytes = result.coverage_info.corpus_size_bytes\n   875→    coverage_info.corpus_location = result.coverage_info.corpus_location\n   876→    coverage_info.corpus_backup_location = (\n   877→        result.coverage_info.corpus_backup_location)\n   878→    coverage_info.quarantine_size_units = (\n   879→        result.coverage_info.quarantine_size_units)\n   880→    coverage_info.quarantine_size_bytes = (\n   881→        result.coverage_info.quarantine_size_bytes)\n   882→    coverage_info.quarantine_location = result.coverage_info.quarantine_location\n   883→    coverage_info.put()\n   884→\n   885→  try:\n   886→    ndb.transaction(\n   887→        _try_save_coverage_information,\n   888→        retries=data_handler.DEFAULT_FAIL_RETRIES)\n   889→  except Exception as e:\n   890→    raise CorpusPruningException(\n   891→        'Failed to save corpus pruning result: %s.' % repr(e))\n   892→\n   893→\n   894→def choose_cross_pollination_strategy(current_fuzzer_name):\n   895→  \"\"\"Chooses cross pollination strategy. In seperate function to mock for\n   896→    predictable test behaviror.\"\"\"\n   897→  method = random.choice([Pollination.RANDOM, Pollination.TAGGED])\n   898→  if method == Pollination.TAGGED:\n   899→    similar_targets = corpus_tagging.get_similarly_tagged_fuzzers(\n   900→        current_fuzzer_name)\n   901→    if similar_targets:\n   902→      return (Pollination.TAGGED, random.choice(similar_targets.keys()))\n   903→\n   904→  return (Pollination.RANDOM, None)\n   905→\n   906→\n   907→def execute_task(full_fuzzer_name, job_type):\n   908→  \"\"\"Execute corpus pruning task.\"\"\"\n   909→  fuzz_target = data_handler.get_fuzz_target(full_fuzzer_name)\n   910→  task_name = 'corpus_pruning_%s_%s' % (full_fuzzer_name, job_type)\n   911→  revision = 0  # Trunk revision\n   912→\n   913→  # Get status of last execution.\n   914→  last_execution_metadata = data_handler.get_task_status(task_name)\n   915→  last_execution_failed = (\n   916→      last_execution_metadata and\n   917→      last_execution_metadata.status == data_types.TaskState.ERROR)\n   918→\n   919→  # Make sure we're the only instance running for the given fuzzer and\n   920→  # job_type.\n   921→  if not data_handler.update_task_status(task_name,\n   922→                                         data_types.TaskState.STARTED):\n   923→    logs.log('A previous corpus pruning task is still running, exiting.')\n   924→    return\n   925→\n   926→  # Setup fuzzer and data bundle.\n   927→  if not setup.update_fuzzer_and_data_bundles(fuzz_target.engine):\n   928→    raise CorpusPruningException(\n   929→        'Failed to set up fuzzer %s.' % fuzz_target.engine)\n   930→\n   931→  cross_pollination_method, tag = choose_cross_pollination_strategy(\n   932→      full_fuzzer_name)\n   933→\n   934→  # TODO(unassigned): Use coverage information for better selection here.\n   935→  cross_pollinate_fuzzers = _get_cross_pollinate_fuzzers(\n   936→      fuzz_target.engine, full_fuzzer_name, cross_pollination_method, tag)\n   937→\n   938→  context = Context(fuzz_target, cross_pollinate_fuzzers,\n   939→                    cross_pollination_method, tag)\n   940→\n   941→  # Copy global blacklist into local suppressions file if LSan is enabled.\n   942→  is_lsan_enabled = environment.get_value('LSAN')\n   943→  if is_lsan_enabled:\n   944→    # TODO(ochang): Copy this to untrusted worker.\n   945→    leak_blacklist.copy_global_to_local_blacklist()\n   946→\n   947→  try:\n   948→    result = do_corpus_pruning(context, last_execution_failed, revision)\n   949→    _record_cross_pollination_stats(result.cross_pollination_stats)\n   950→    _save_coverage_information(context, result)\n   951→    _process_corpus_crashes(context, result)\n   952→  except Exception:\n   953→    logs.log_error('Corpus pruning failed.')\n   954→    data_handler.update_task_status(task_name, data_types.TaskState.ERROR)\n   955→    return\n   956→  finally:\n   957→    context.cleanup()\n   958→\n   959→  data_handler.update_task_status(task_name, data_types.TaskState.FINISHED)\n   960→\n\n<system-reminder>\nWhenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.\n</system-reminder>\n"}]},"parent_tool_use_id":null,"session_id":"6939fe31-d677-4c31-9307-4891924f760e","uuid":"a404316e-b13a-4fbe-b943-425a9234625f","tool_use_result":{"type":"text","file":{"filePath":"/Users/emmayu/crqbench/projects/clusterfuzz/src/python/bot/tasks/corpus_pruning_task.py","content":"# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Corpus pruning task.\"\"\"\n\nfrom builtins import object\nfrom builtins import zip\n\nimport collections\nimport datetime\nimport os\nimport random\nimport shutil\n\nfrom google.cloud import ndb\n\nfrom base import utils\nfrom bot.fuzzers import engine\nfrom bot.fuzzers import engine_common\nfrom bot.fuzzers import options\nfrom bot.tasks import setup\nfrom bot.tasks import task_creation\nfrom build_management import build_manager\nfrom crash_analysis import crash_analyzer\nfrom crash_analysis.stack_parsing import stack_analyzer\nfrom crash_analysis.stack_parsing import stack_symbolizer\nfrom datastore import corpus_tagging\nfrom datastore import data_handler\nfrom datastore import data_types\nfrom datastore import fuzz_target_utils\nfrom enum import Enum\nfrom fuzzing import corpus_manager\nfrom fuzzing import leak_blacklist\nfrom google_cloud_utils import big_query\nfrom google_cloud_utils import blobs\nfrom google_cloud_utils import storage\nfrom metrics import logs\nfrom system import archive\nfrom system import environment\nfrom system import shell\n\n# TODO(ochang): Move common libFuzzer code from fuzzer into CF.\n\n# Redzone size for running testcase.\nDEFAULT_REDZONE = 32\n\n# Minimum redzone size for use during merging.\nMIN_REDZONE = 16\n\n# Timeout for corpus pruning. Note that our priority is to make sure fuzzer's\n# own corpus is minimized. If time remains, we spend time on using units from\n# shared corpus. This combined timeout should be lower than task lease timeout.\nCORPUS_PRUNING_TIMEOUT = 22 * 60 * 60\n\n# Time to allow libFuzzer to timeout on its own.\nSINGLE_UNIT_TIMEOUT = 5\nTIMEOUT_FLAG = '-timeout=%d' % SINGLE_UNIT_TIMEOUT\n\n# Corpus files limit for cases when corpus pruning task failed in the last\n# execution.\nCORPUS_FILES_LIMIT_FOR_FAILURES = 10000\n\n# Corpus total size limit for cases when corpus pruning task failed in the last\n# execution.\nCORPUS_SIZE_LIMIT_FOR_FAILURES = 2 * 1024 * 1024 * 1024  # 2 GB.\n\n# Maximum number of units to restore from quarantine in one run.\nMAX_QUARANTINE_UNITS_TO_RESTORE = 128\n\n# Memory limits for testcase.\nRSS_LIMIT = 2560\nRSS_LIMIT_MB_FLAG = '-rss_limit_mb=%d'\n\n# Flag to disable leak checking.\nDISABLE_LEAK_CHECK_FLAG = '-detect_leaks=0'\n\n# Flag to do value profile during merges.\nUSE_VALUE_PROFILE_FLAG = '-use_value_profile=%d'\n\n# Corpus size limit to allow use of value profile. This prevents corpus from\n# growing unbounded.\nCORPUS_SIZE_LIMIT_FOR_VALUE_PROFILE = 50000\n\n# Longer than default sync timeout to fix broken (overly large) corpora without\n# losing coverage.\nSYNC_TIMEOUT = 2 * 60 * 60\n\n# Number of fuzz targets whose backup corpus is used to cross pollinate with our\n# current fuzz target corpus.\nCROSS_POLLINATE_FUZZER_COUNT = 3\n\nCorpusPruningResult = collections.namedtuple('CorpusPruningResult', [\n    'coverage_info', 'crashes', 'fuzzer_binary_name', 'revision',\n    'cross_pollination_stats'\n])\n\nCorpusCrash = collections.namedtuple('CorpusCrash', [\n    'crash_state',\n    'crash_type',\n    'crash_address',\n    'crash_stacktrace',\n    'unit_path',\n    'security_flag',\n])\n\nCrossPollinationStats = collections.namedtuple('CrossPollinationStats', [\n    'project_qualified_name', 'method', 'sources', 'tags',\n    'initial_corpus_size', 'corpus_size', 'initial_edge_coverage',\n    'edge_coverage', 'initial_feature_coverage', 'feature_coverage'\n])\n\n\nclass Pollination(str, Enum):\n  RANDOM = 'random'\n  TAGGED = 'tagged'\n\n\ndef _get_corpus_file_paths(corpus_path):\n  \"\"\"Return full paths to corpus files in |corpus_path|.\"\"\"\n  return [\n      os.path.join(corpus_path, filename)\n      for filename in os.listdir(corpus_path)\n  ]\n\n\ndef _limit_corpus_size(corpus_url):\n  \"\"\"Limit number of files and size of a corpus.\"\"\"\n  corpus_count = 0\n  corpus_size = 0\n  deleted_corpus_count = 0\n  bucket, _ = storage.get_bucket_name_and_path(corpus_url)\n  for corpus_file in storage.get_blobs(corpus_url):\n    corpus_count += 1\n    corpus_size += corpus_file['size']\n    if (corpus_count > CORPUS_FILES_LIMIT_FOR_FAILURES or\n        corpus_size > CORPUS_SIZE_LIMIT_FOR_FAILURES):\n      path_to_delete = storage.get_cloud_storage_file_path(\n          bucket, corpus_file['name'])\n      storage.delete(path_to_delete)\n      deleted_corpus_count += 1\n\n  if deleted_corpus_count:\n    logs.log('Removed %d files from oversized corpus: %s.' %\n             (deleted_corpus_count, corpus_url))\n\n\ndef _get_time_remaining(start_time):\n  \"\"\"Return time remaining.\"\"\"\n  time_used = int((datetime.datetime.utcnow() - start_time).total_seconds())\n  return CORPUS_PRUNING_TIMEOUT - time_used\n\n\nclass CorpusPruningException(Exception):\n  \"\"\"Corpus pruning exception.\"\"\"\n\n\nclass CrossPollinateFuzzer(object):\n  \"\"\"Cross Pollinate Fuzzer.\"\"\"\n\n  def __init__(self, fuzz_target, backup_bucket_name, corpus_engine_name):\n    self.fuzz_target = fuzz_target\n    self.backup_bucket_name = backup_bucket_name\n    self.corpus_engine_name = corpus_engine_name\n\n\nclass Context(object):\n  \"\"\"Pruning state.\"\"\"\n\n  def __init__(self,\n               fuzz_target,\n               cross_pollinate_fuzzers,\n               cross_pollination_method=Pollination.RANDOM,\n               tag=None):\n\n    self.fuzz_target = fuzz_target\n    self.cross_pollinate_fuzzers = cross_pollinate_fuzzers\n    self.cross_pollination_method = cross_pollination_method\n    self.tag = tag\n\n    self.merge_tmp_dir = None\n    self.engine = engine.get(self.fuzz_target.engine)\n    if not self.engine:\n      raise CorpusPruningException('Engine {} not found'.format(engine))\n\n    self._created_directories = []\n\n    # Set up temporary directories where corpora will be synced to.\n    # Initial synced corpus.\n    self.initial_corpus_path = self._create_temp_corpus_directory(\n        '%s_initial_corpus' % self.fuzz_target.project_qualified_name())\n    # Minimized corpus.\n    self.minimized_corpus_path = self._create_temp_corpus_directory(\n        '%s_minimized_corpus' % self.fuzz_target.project_qualified_name())\n    # Synced quarantine corpus.\n    self.quarantine_corpus_path = self._create_temp_corpus_directory(\n        '%s_quarantine' % self.fuzz_target.project_qualified_name())\n    # Synced shared corpus.\n    self.shared_corpus_path = self._create_temp_corpus_directory(\n        '%s_shared' % self.fuzz_target.project_qualified_name())\n    # Bad units.\n    self.bad_units_path = self._create_temp_corpus_directory(\n        '%s_bad_units' % self.fuzz_target.project_qualified_name())\n    self.merge_tmp_dir = self._create_temp_corpus_directory('merge_workdir')\n\n    self.corpus = corpus_manager.FuzzTargetCorpus(\n        self.fuzz_target.engine, self.fuzz_target.project_qualified_name())\n    self.quarantine_corpus = corpus_manager.FuzzTargetCorpus(\n        self.fuzz_target.engine,\n        self.fuzz_target.project_qualified_name(),\n        quarantine=True)\n\n    shared_corpus_bucket = environment.get_value('SHARED_CORPUS_BUCKET')\n    self.shared_corpus = corpus_manager.GcsCorpus(shared_corpus_bucket)\n\n  def restore_quarantined_units(self):\n    \"\"\"Restore units from the quarantine.\"\"\"\n    logs.log('Restoring units from quarantine.')\n    # Limit the number of quarantine units to restore, in case there are a lot.\n    quarantine_unit_paths = _get_corpus_file_paths(self.quarantine_corpus_path)\n    if len(quarantine_unit_paths) > MAX_QUARANTINE_UNITS_TO_RESTORE:\n      quarantine_unit_paths = random.sample(quarantine_unit_paths,\n                                            MAX_QUARANTINE_UNITS_TO_RESTORE)\n\n    for unit_path in quarantine_unit_paths:\n      unit_filename = os.path.basename(unit_path)\n      shutil.move(unit_path,\n                  os.path.join(self.initial_corpus_path, unit_filename))\n\n  def _create_temp_corpus_directory(self, name):\n    \"\"\"Create temporary corpus directory. Returns path to the created\n    directory.\"\"\"\n    testcases_directory = environment.get_value('FUZZ_INPUTS_DISK')\n    directory_path = os.path.join(testcases_directory, name)\n    shell.create_directory(directory_path)\n    self._created_directories.append(directory_path)\n\n    return directory_path\n\n  def sync_to_disk(self):\n    \"\"\"Sync required corpora to disk.\"\"\"\n    if not self.corpus.rsync_to_disk(\n        self.initial_corpus_path, timeout=SYNC_TIMEOUT):\n      raise CorpusPruningException('Failed to sync corpus to disk.')\n\n    if not self.quarantine_corpus.rsync_to_disk(self.quarantine_corpus_path):\n      logs.log_error(\n          'Failed to sync quarantine corpus to disk.',\n          fuzz_target=self.fuzz_target)\n\n    if not self.shared_corpus.rsync_to_disk(self.shared_corpus_path):\n      logs.log_error(\n          'Failed to sync shared corpus to disk.', fuzz_target=self.fuzz_target)\n\n    self._cross_pollinate_other_fuzzer_corpuses()\n\n  def sync_to_gcs(self):\n    \"\"\"Sync corpora to GCS post merge.\"\"\"\n    if not self.corpus.rsync_from_disk(self.minimized_corpus_path):\n      raise CorpusPruningException('Failed to sync minimized corpus to gcs.')\n\n  def cleanup(self):\n    \"\"\"Cleanup state.\"\"\"\n    for path in self._created_directories:\n      shell.remove_directory(path)\n\n  def _cross_pollinate_other_fuzzer_corpuses(self):\n    \"\"\"Add other fuzzer corpuses to shared corpus path for cross-pollination.\"\"\"\n    corpus_backup_date = utils.utcnow().date() - datetime.timedelta(\n        days=data_types.CORPUS_BACKUP_PUBLIC_LOOKBACK_DAYS)\n\n    for cross_pollinate_fuzzer in self.cross_pollinate_fuzzers:\n      project_qualified_name = (\n          cross_pollinate_fuzzer.fuzz_target.project_qualified_name())\n      backup_bucket_name = cross_pollinate_fuzzer.backup_bucket_name\n      corpus_engine_name = cross_pollinate_fuzzer.corpus_engine_name\n\n      corpus_backup_url = corpus_manager.gcs_url_for_backup_file(\n          backup_bucket_name, corpus_engine_name, project_qualified_name,\n          corpus_backup_date)\n      corpus_backup_local_filename = '%s-%s' % (\n          project_qualified_name, os.path.basename(corpus_backup_url))\n      corpus_backup_local_path = os.path.join(self.shared_corpus_path,\n                                              corpus_backup_local_filename)\n\n      if not storage.exists(corpus_backup_url, ignore_errors=True):\n        # This can happen in cases when a new fuzz target is checked in or if\n        # missed to capture a backup for a particular day (for OSS-Fuzz, this\n        # will result in a 403 instead of 404 since that GCS path belongs to\n        # other project). So, just log a warning for debugging purposes only.\n        logs.log_warn(\n            'Corpus backup does not exist, ignoring: %s.' % corpus_backup_url)\n        continue\n\n      if not storage.copy_file_from(corpus_backup_url,\n                                    corpus_backup_local_path):\n        continue\n\n      corpus_backup_output_directory = os.path.join(self.shared_corpus_path,\n                                                    project_qualified_name)\n      shell.create_directory(corpus_backup_output_directory)\n      result = archive.unpack(corpus_backup_local_path,\n                              corpus_backup_output_directory)\n      shell.remove_file(corpus_backup_local_path)\n\n      if result:\n        logs.log(\n            'Corpus backup url %s successfully unpacked into shared corpus.' %\n            corpus_backup_url)\n      else:\n        logs.log_error(\n            'Failed to unpack corpus backup from url %s.' % corpus_backup_url)\n\n\nclass Runner(object):\n  \"\"\"Runner for libFuzzer.\"\"\"\n\n  def __init__(self, build_directory, context):\n    self.build_directory = build_directory\n    self.context = context\n\n    self.target_path = engine_common.find_fuzzer_path(\n        self.build_directory, self.context.fuzz_target.binary)\n    if not self.target_path:\n      raise CorpusPruningException(\n          'Failed to get fuzzer path for %s.' % self.context.fuzz_target.binary)\n\n    self.fuzzer_options = options.get_fuzz_target_options(self.target_path)\n\n  def get_libfuzzer_flags(self):\n    \"\"\"Get default libFuzzer options.\"\"\"\n    if self.fuzzer_options:\n      libfuzzer_arguments = self.fuzzer_options.get_engine_arguments(\n          'libfuzzer')\n\n      # Allow some flags to be used from .options file for single unit testing.\n      # Allow specifying a lower rss_limit.\n      rss_limit = libfuzzer_arguments.get('rss_limit_mb', constructor=int)\n      if not rss_limit or rss_limit > RSS_LIMIT:\n        rss_limit = RSS_LIMIT\n\n      # Some targets might falsely report leaks all the time, so allow this to\n      # be disabled.\n      detect_leaks = libfuzzer_arguments.get('detect_leaks', default='1')\n      arguments = [\n          RSS_LIMIT_MB_FLAG % rss_limit,\n          '-detect_leaks=%s' % detect_leaks, TIMEOUT_FLAG\n      ]\n    else:\n      arguments = [RSS_LIMIT_MB_FLAG % RSS_LIMIT, TIMEOUT_FLAG]\n\n    corpus_size = shell.get_directory_file_count(\n        self.context.initial_corpus_path)\n    use_value_profile = int(corpus_size <= CORPUS_SIZE_LIMIT_FOR_VALUE_PROFILE)\n    arguments.append(USE_VALUE_PROFILE_FLAG % use_value_profile)\n\n    return arguments\n\n  def process_sanitizer_options(self):\n    \"\"\"Process sanitizer options overrides.\"\"\"\n    if not self.fuzzer_options:\n      return\n\n    # Only need to look as ASan, as that's what we prune with.\n    overrides = self.fuzzer_options.get_asan_options()\n    if not overrides:\n      return\n\n    asan_options = environment.get_memory_tool_options('ASAN_OPTIONS')\n    asan_options.update(overrides)\n    environment.set_memory_tool_options('ASAN_OPTIONS', asan_options)\n\n  def reproduce(self, input_path, arguments, max_time):\n    return self.context.engine.reproduce(self.target_path, input_path,\n                                         arguments, max_time)\n\n  def minimize_corpus(self, arguments, input_dirs, output_dir, reproducers_dir,\n                      max_time):\n    return self.context.engine.minimize_corpus(self.target_path, arguments,\n                                               input_dirs, output_dir,\n                                               reproducers_dir, max_time)\n\n\nclass CorpusPruner(object):\n  \"\"\"Class that handles corpus pruning.\"\"\"\n\n  def __init__(self, runner):\n    self.runner = runner\n    self.context = self.runner.context\n\n  def _run_single_unit(self, unit_path):\n    \"\"\"Run a single unit, and return the result.\"\"\"\n    arguments = self.runner.get_libfuzzer_flags()\n    return self.runner.reproduce(unit_path, arguments, SINGLE_UNIT_TIMEOUT)\n\n  def _quarantine_unit(self, unit_path, quarantine_corpus_path):\n    \"\"\"Moves the given unit to the quarantine, and returns the path to the unit\n    in the quarantine.\"\"\"\n    quarantined_unit_path = os.path.join(quarantine_corpus_path,\n                                         os.path.basename(unit_path))\n    shutil.move(unit_path, quarantined_unit_path)\n\n    return quarantined_unit_path\n\n  def process_bad_units(self, bad_units_path, quarantine_corpus_path, crashes):\n    \"\"\"Process bad units found during merge.\"\"\"\n    # TODO(ochang): A lot of this function is similar to parts of fuzz_task.\n    # Ideally fuzz_task can be refactored in a way that lets us share the common\n    # code.\n\n    environment.reset_current_memory_tool_options(redzone_size=DEFAULT_REDZONE)\n    self.runner.process_sanitizer_options()\n\n    logs.log('Processing bad units.')\n    corpus_file_paths = _get_corpus_file_paths(bad_units_path)\n    num_bad_units = 0\n\n    # Run each corpus item individually.\n    for i, unit_path in enumerate(corpus_file_paths, 1):\n      if i % 100 == 0:\n        logs.log('Up to %d' % i)\n\n      unit_name = os.path.basename(unit_path)\n      if unit_name.startswith('timeout-') or unit_name.startswith('oom-'):\n        # Don't waste time re-running timeout or oom testcases.\n        unit_path = self._quarantine_unit(unit_path, quarantine_corpus_path)\n        num_bad_units += 1\n        continue\n\n      try:\n        result = self._run_single_unit(unit_path)\n      except engine.TimeoutError:\n        # Slow unit. Quarantine it.\n        unit_path = self._quarantine_unit(unit_path, quarantine_corpus_path)\n        num_bad_units += 1\n        continue\n\n      if not crash_analyzer.is_memory_tool_crash(result.output):\n        # Didn't crash.\n        continue\n\n      # Get memory tool crash information.\n      state = stack_analyzer.get_crash_data(result.output, symbolize_flag=True)\n\n      # Crashed or caused a leak. Quarantine it.\n      unit_path = self._quarantine_unit(unit_path, quarantine_corpus_path)\n      num_bad_units += 1\n\n      if crash_analyzer.ignore_stacktrace(state.crash_stacktrace):\n        continue\n\n      # Local de-duplication.\n      if state.crash_state not in crashes:\n        security_flag = crash_analyzer.is_security_issue(\n            state.crash_stacktrace, state.crash_type, state.crash_address)\n        crashes[state.crash_state] = CorpusCrash(\n            state.crash_state, state.crash_type, state.crash_address,\n            state.crash_stacktrace, unit_path, security_flag)\n\n    logs.log('Found %d bad units, %d unique crashes.' % (num_bad_units,\n                                                         len(crashes)))\n\n  def run(self, initial_corpus_path, minimized_corpus_path, bad_units_path):\n    \"\"\"Run corpus pruning. Output result to directory.\"\"\"\n    if not shell.get_directory_file_count(initial_corpus_path):\n      # Empty corpus, nothing to do.\n      return None\n\n    # Set memory tool options and fuzzer arguments.\n    engine_common.unpack_seed_corpus_if_needed(\n        self.runner.target_path, initial_corpus_path, force_unpack=True)\n\n    environment.reset_current_memory_tool_options(\n        redzone_size=MIN_REDZONE, leaks=True)\n    self.runner.process_sanitizer_options()\n    additional_args = self.runner.get_libfuzzer_flags()\n\n    # Execute fuzzer with arguments for corpus pruning.\n    logs.log('Running merge...')\n    try:\n      result = self.runner.minimize_corpus(\n          additional_args, [initial_corpus_path], minimized_corpus_path,\n          bad_units_path, CORPUS_PRUNING_TIMEOUT)\n    except engine.TimeoutError as e:\n      raise CorpusPruningException(\n          'Corpus pruning timed out while minimizing corpus\\n' + repr(e))\n    except engine.Error as e:\n      raise CorpusPruningException(\n          'Corpus pruning failed to minimize corpus\\n' + repr(e))\n\n    symbolized_output = stack_symbolizer.symbolize_stacktrace(result.logs)\n\n    # Sanity check that there are files in minimized corpus after merging.\n    if not shell.get_directory_file_count(minimized_corpus_path):\n      raise CorpusPruningException(\n          'Corpus pruning failed to minimize corpus\\n' + symbolized_output)\n\n    logs.log('Corpus merge finished successfully.', output=symbolized_output)\n\n    return result.stats\n\n\nclass CrossPollinator(object):\n  \"\"\"Cross pollination.\"\"\"\n\n  def __init__(self, runner):\n    self.runner = runner\n    self.context = self.runner.context\n\n  def run(self, timeout):\n    \"\"\"Merge testcases from corpus from other fuzz targets.\"\"\"\n    if not shell.get_directory_file_count(self.context.shared_corpus_path):\n      logs.log('No files found in shared corpus, skip merge.')\n      return None\n\n    # Run pruning on the shared corpus and log the result in case of error.\n    logs.log('Merging shared corpus...')\n    environment.reset_current_memory_tool_options(redzone_size=DEFAULT_REDZONE)\n    self.runner.process_sanitizer_options()\n\n    additional_args = self.runner.get_libfuzzer_flags()\n\n    try:\n      result = self.runner.minimize_corpus(additional_args,\n                                           [self.context.shared_corpus_path],\n                                           self.context.minimized_corpus_path,\n                                           self.context.bad_units_path, timeout)\n      symbolized_output = stack_symbolizer.symbolize_stacktrace(result.logs)\n      logs.log(\n          'Shared corpus merge finished successfully.',\n          output=symbolized_output)\n    except engine.TimeoutError as e:\n      # Other cross pollinated fuzzer corpuses can have unexpected test cases\n      # that time us out. This is expected, so bail out.\n      logs.log_warn('Corpus pruning timed out while merging shared corpus\\n' +\n                    repr(e))\n      return None\n    except engine.Error as e:\n      # Other cross pollinated fuzzer corpuses can be large, so we can run out\n      # of disk space and exception out. This is expected, so bail out.\n      logs.log_warn('Corpus pruning failed to merge shared corpus\\n' + repr(e))\n      return None\n\n    return result.stats\n\n\ndef _record_cross_pollination_stats(stats):\n  \"\"\"Log stats about cross pollination in BigQuery.\"\"\"\n  # If no stats were gathered due to a timeout or lack of corpus, return.\n  if not stats:\n    return\n\n  bigquery_row = {\n      'project_qualified_name': stats.project_qualified_name,\n      'method': stats.method,\n      'sources': stats.sources,\n      'tags': stats.tags if stats.tags else '',\n      'initial_corpus_size': stats.initial_corpus_size,\n      'corpus_size': stats.corpus_size,\n      'initial_edge_coverage': stats.initial_edge_coverage,\n      'edge_coverage': stats.edge_coverage,\n      'initial_feature_coverage': stats.initial_feature_coverage,\n      'feature_coverage': stats.feature_coverage\n  }\n\n  # BigQuery not available in local development. This is necessary because the\n  # untrusted runner is in a separate process and can't be easily mocked.\n  # Check here instead of earlier to test as much of the function as we can.\n  if environment.get_value('LOCAL_DEVELOPMENT') or environment.get_value(\n      'PY_UNITTESTS'):\n    return\n\n  client = big_query.Client(\n      dataset_id='main', table_id='cross_pollination_statistics')\n  client.insert([big_query.Insert(row=bigquery_row, insert_id=None)])\n\n\ndef do_corpus_pruning(context, last_execution_failed, revision):\n  \"\"\"Run corpus pruning.\"\"\"\n  # Set |FUZZ_TARGET| environment variable to help with unarchiving only fuzz\n  # target and its related files.\n  environment.set_value('FUZZ_TARGET', context.fuzz_target.binary)\n\n  if environment.is_trusted_host():\n    from bot.untrusted_runner import tasks_host\n    return tasks_host.do_corpus_pruning(context, last_execution_failed,\n                                        revision)\n\n  build_setup_result = build_manager.setup_build(revision=revision)\n  build_directory = environment.get_value('BUILD_DIR')\n  if not build_setup_result or not build_directory:\n    raise CorpusPruningException('Failed to setup build.')\n\n  start_time = datetime.datetime.utcnow()\n  runner = Runner(build_directory, context)\n  pruner = CorpusPruner(runner)\n  fuzzer_binary_name = os.path.basename(runner.target_path)\n\n  # If our last execution failed, shrink to a randomized corpus of usable size\n  # to prevent corpus from growing unbounded and recurring failures when trying\n  # to minimize it.\n  if last_execution_failed:\n    for corpus_url in [\n        context.corpus.get_gcs_url(),\n        context.quarantine_corpus.get_gcs_url()\n    ]:\n      _limit_corpus_size(corpus_url)\n\n  # Get initial corpus to process from GCS.\n  context.sync_to_disk()\n  initial_corpus_size = shell.get_directory_file_count(\n      context.initial_corpus_path)\n\n  # Restore a small batch of quarantined units back to corpus.\n  context.restore_quarantined_units()\n\n  # Shrink to a minimized corpus using corpus merge.\n  pruner_stats = pruner.run(context.initial_corpus_path,\n                            context.minimized_corpus_path,\n                            context.bad_units_path)\n\n  # Sync minimized corpus back to GCS.\n  context.sync_to_gcs()\n\n  # Create corpus backup.\n  backup_bucket = environment.get_value('BACKUP_BUCKET')\n  corpus_backup_url = corpus_manager.backup_corpus(\n      backup_bucket, context.corpus, context.minimized_corpus_path)\n\n  minimized_corpus_size_units = shell.get_directory_file_count(\n      context.minimized_corpus_path)\n  minimized_corpus_size_bytes = shell.get_directory_size(\n      context.minimized_corpus_path)\n\n  logs.log('Corpus pruned from %d to %d units.' % (initial_corpus_size,\n                                                   minimized_corpus_size_units))\n\n  # Process bad units found during merge.\n  # Mapping of crash state -> CorpusCrash\n  crashes = {}\n  pruner.process_bad_units(context.bad_units_path,\n                           context.quarantine_corpus_path, crashes)\n  context.quarantine_corpus.rsync_from_disk(context.quarantine_corpus_path)\n\n  # Store corpus stats into CoverageInformation entity.\n  project_qualified_name = context.fuzz_target.project_qualified_name()\n  today = datetime.datetime.utcnow().date()\n  coverage_info = data_types.CoverageInformation(\n      fuzzer=project_qualified_name, date=today)\n\n  quarantine_corpus_size = shell.get_directory_file_count(\n      context.quarantine_corpus_path)\n  quarantine_corpus_dir_size = shell.get_directory_size(\n      context.quarantine_corpus_path)\n\n  # Save the minimize corpus size before cross pollination to put in BigQuery.\n  pre_pollination_corpus_size = minimized_corpus_size_units\n\n  # Populate coverage stats.\n  coverage_info.corpus_size_units = minimized_corpus_size_units\n  coverage_info.corpus_size_bytes = minimized_corpus_size_bytes\n  coverage_info.quarantine_size_units = quarantine_corpus_size\n  coverage_info.quarantine_size_bytes = quarantine_corpus_dir_size\n  coverage_info.corpus_backup_location = corpus_backup_url\n  coverage_info.corpus_location = context.corpus.get_gcs_url()\n  coverage_info.quarantine_location = context.quarantine_corpus.get_gcs_url()\n\n  # Calculate remaining time to use for shared corpus merging.\n  time_remaining = _get_time_remaining(start_time)\n  if time_remaining <= 0:\n    logs.log_warn('Not enough time for shared corpus merging.')\n    return None\n\n  cross_pollinator = CrossPollinator(runner)\n  pollinator_stats = cross_pollinator.run(time_remaining)\n\n  context.sync_to_gcs()\n\n  # Update corpus size stats.\n  minimized_corpus_size_units = shell.get_directory_file_count(\n      context.minimized_corpus_path)\n  minimized_corpus_size_bytes = shell.get_directory_size(\n      context.minimized_corpus_path)\n  coverage_info.corpus_size_units = minimized_corpus_size_units\n  coverage_info.corpus_size_bytes = minimized_corpus_size_bytes\n\n  logs.log('Finished.')\n\n  sources = ','.join([\n      fuzzer.fuzz_target.project_qualified_name()\n      for fuzzer in context.cross_pollinate_fuzzers\n  ])\n\n  cross_pollination_stats = None\n  if pruner_stats and pollinator_stats:\n    cross_pollination_stats = CrossPollinationStats(\n        project_qualified_name, context.cross_pollination_method, sources,\n        context.tag, initial_corpus_size, pre_pollination_corpus_size,\n        pruner_stats['edge_coverage'], pollinator_stats['edge_coverage'],\n        pruner_stats['feature_coverage'], pollinator_stats['feature_coverage'])\n\n  return CorpusPruningResult(\n      coverage_info=coverage_info,\n      crashes=list(crashes.values()),\n      fuzzer_binary_name=fuzzer_binary_name,\n      revision=environment.get_value('APP_REVISION'),\n      cross_pollination_stats=cross_pollination_stats)\n\n\ndef _process_corpus_crashes(context, result):\n  \"\"\"Process crashes found in the corpus.\"\"\"\n  # Default Testcase entity values.\n  crash_revision = result.revision\n  job_type = environment.get_value('JOB_NAME')\n  minimized_arguments = '%TESTCASE% ' + context.fuzz_target.binary\n  project_name = data_handler.get_project_name(job_type)\n\n  comment = 'Fuzzer %s generated corpus testcase crashed (r%s)' % (\n      context.fuzz_target.project_qualified_name(), crash_revision)\n\n  # Generate crash reports.\n  for crash in result.crashes:\n    existing_testcase = data_handler.find_testcase(\n        project_name, crash.crash_type, crash.crash_state, crash.security_flag)\n    if existing_testcase:\n      continue\n\n    # Upload/store testcase.\n    if environment.is_trusted_host():\n      from bot.untrusted_runner import file_host\n      unit_path = os.path.join(context.bad_units_path,\n                               os.path.basename(crash.unit_path))\n      # Prevent the worker from escaping out of |context.bad_units_path|.\n      if not file_host.is_directory_parent(unit_path, context.bad_units_path):\n        raise CorpusPruningException('Invalid units path from worker.')\n\n      file_host.copy_file_from_worker(crash.unit_path, unit_path)\n    else:\n      unit_path = crash.unit_path\n\n    with open(unit_path, 'rb') as f:\n      key = blobs.write_blob(f)\n\n    # Set the absolute_path property of the Testcase to a file in FUZZ_INPUTS\n    # instead of the local quarantine directory.\n    absolute_testcase_path = os.path.join(\n        environment.get_value('FUZZ_INPUTS'), 'testcase')\n\n    testcase_id = data_handler.store_testcase(\n        crash=crash,\n        fuzzed_keys=key,\n        minimized_keys='',\n        regression='',\n        fixed='',\n        one_time_crasher_flag=False,\n        crash_revision=crash_revision,\n        comment=comment,\n        absolute_path=absolute_testcase_path,\n        fuzzer_name=context.fuzz_target.engine,\n        fully_qualified_fuzzer_name=context.fuzz_target.fully_qualified_name(),\n        job_type=job_type,\n        archived=False,\n        archive_filename='',\n        binary_flag=True,\n        http_flag=False,\n        gestures=None,\n        redzone=DEFAULT_REDZONE,\n        disable_ubsan=False,\n        minidump_keys=None,\n        window_argument=None,\n        timeout_multiplier=1.0,\n        minimized_arguments=minimized_arguments)\n\n    # Set fuzzer_binary_name in testcase metadata.\n    testcase = data_handler.get_testcase_by_id(testcase_id)\n    testcase.set_metadata('fuzzer_binary_name', result.fuzzer_binary_name)\n\n    issue_metadata = engine_common.get_all_issue_metadata_for_testcase(testcase)\n    if issue_metadata:\n      for key, value in issue_metadata.items():\n        testcase.set_metadata(key, value, update_testcase=False)\n\n      testcase.put()\n\n    # Create additional tasks for testcase (starting with minimization).\n    testcase = data_handler.get_testcase_by_id(testcase_id)\n    task_creation.create_tasks(testcase)\n\n\ndef _select_targets_and_jobs_for_pollination(engine_name, current_fuzzer_name,\n                                             method, tag):\n  \"\"\"Select jobs to use for cross pollination.\"\"\"\n  target_jobs = list(fuzz_target_utils.get_fuzz_target_jobs(engine=engine_name))\n\n  if method == Pollination.TAGGED:\n    similar_tagged_targets = [\n        target.fully_qualified_fuzz_target_name\n        for target in corpus_tagging.get_targets_with_tag(tag)\n        if target.fully_qualified_fuzz_target_name != current_fuzzer_name\n    ]\n    # Intersect target_jobs and similar_tagged_targets on fully qualified\n    # fuzz target name.\n    target_jobs = [\n        target for target in target_jobs\n        if target.fuzz_target_name in similar_tagged_targets\n    ]\n\n  targets = fuzz_target_utils.get_fuzz_targets_for_target_jobs(target_jobs)\n\n  targets_and_jobs = [(target, target_job)\n                      for target, target_job in zip(targets, target_jobs)\n                      if target_job.fuzz_target_name != current_fuzzer_name]\n  selected_targets_and_jobs = random.SystemRandom().sample(\n      targets_and_jobs, min(\n          len(targets_and_jobs), CROSS_POLLINATE_FUZZER_COUNT))\n\n  return selected_targets_and_jobs\n\n\ndef _get_cross_pollinate_fuzzers(engine_name, current_fuzzer_name, method, tag):\n  \"\"\"Return a list of fuzzer objects to use for cross pollination.\"\"\"\n  cross_pollinate_fuzzers = []\n\n  selected_targets_and_jobs = _select_targets_and_jobs_for_pollination(\n      engine_name, current_fuzzer_name, method, tag)\n\n  default_backup_bucket = utils.default_backup_bucket()\n  for target, target_job in selected_targets_and_jobs:\n    job = data_types.Job.query(data_types.Job.name == target_job.job).get()\n    if not job:\n      continue\n\n    job_environment = job.get_environment()\n    backup_bucket_name = job_environment.get('BACKUP_BUCKET',\n                                             default_backup_bucket)\n    if not backup_bucket_name:\n      continue\n    corpus_engine_name = job_environment.get('CORPUS_FUZZER_NAME_OVERRIDE',\n                                             engine_name)\n\n    cross_pollinate_fuzzers.append(\n        CrossPollinateFuzzer(\n            target,\n            backup_bucket_name,\n            corpus_engine_name,\n        ))\n\n  return cross_pollinate_fuzzers\n\n\ndef _save_coverage_information(context, result):\n  \"\"\"Saves coverage information in datastore using an atomic transaction.\"\"\"\n\n  # Use ndb.transaction with retries below to mitigate risk of a race condition.\n  def _try_save_coverage_information():\n    \"\"\"Implements save_coverage_information function.\"\"\"\n    coverage_info = data_handler.get_coverage_information(\n        context.fuzz_target.project_qualified_name(),\n        result.coverage_info.date,\n        create_if_needed=True)\n\n    # Intentionally skip edge and function coverage values as those would come\n    # from fuzzer coverage cron task (see src/go/server/cron/coverage.go).\n    coverage_info.corpus_size_units = result.coverage_info.corpus_size_units\n    coverage_info.corpus_size_bytes = result.coverage_info.corpus_size_bytes\n    coverage_info.corpus_location = result.coverage_info.corpus_location\n    coverage_info.corpus_backup_location = (\n        result.coverage_info.corpus_backup_location)\n    coverage_info.quarantine_size_units = (\n        result.coverage_info.quarantine_size_units)\n    coverage_info.quarantine_size_bytes = (\n        result.coverage_info.quarantine_size_bytes)\n    coverage_info.quarantine_location = result.coverage_info.quarantine_location\n    coverage_info.put()\n\n  try:\n    ndb.transaction(\n        _try_save_coverage_information,\n        retries=data_handler.DEFAULT_FAIL_RETRIES)\n  except Exception as e:\n    raise CorpusPruningException(\n        'Failed to save corpus pruning result: %s.' % repr(e))\n\n\ndef choose_cross_pollination_strategy(current_fuzzer_name):\n  \"\"\"Chooses cross pollination strategy. In seperate function to mock for\n    predictable test behaviror.\"\"\"\n  method = random.choice([Pollination.RANDOM, Pollination.TAGGED])\n  if method == Pollination.TAGGED:\n    similar_targets = corpus_tagging.get_similarly_tagged_fuzzers(\n        current_fuzzer_name)\n    if similar_targets:\n      return (Pollination.TAGGED, random.choice(similar_targets.keys()))\n\n  return (Pollination.RANDOM, None)\n\n\ndef execute_task(full_fuzzer_name, job_type):\n  \"\"\"Execute corpus pruning task.\"\"\"\n  fuzz_target = data_handler.get_fuzz_target(full_fuzzer_name)\n  task_name = 'corpus_pruning_%s_%s' % (full_fuzzer_name, job_type)\n  revision = 0  # Trunk revision\n\n  # Get status of last execution.\n  last_execution_metadata = data_handler.get_task_status(task_name)\n  last_execution_failed = (\n      last_execution_metadata and\n      last_execution_metadata.status == data_types.TaskState.ERROR)\n\n  # Make sure we're the only instance running for the given fuzzer and\n  # job_type.\n  if not data_handler.update_task_status(task_name,\n                                         data_types.TaskState.STARTED):\n    logs.log('A previous corpus pruning task is still running, exiting.')\n    return\n\n  # Setup fuzzer and data bundle.\n  if not setup.update_fuzzer_and_data_bundles(fuzz_target.engine):\n    raise CorpusPruningException(\n        'Failed to set up fuzzer %s.' % fuzz_target.engine)\n\n  cross_pollination_method, tag = choose_cross_pollination_strategy(\n      full_fuzzer_name)\n\n  # TODO(unassigned): Use coverage information for better selection here.\n  cross_pollinate_fuzzers = _get_cross_pollinate_fuzzers(\n      fuzz_target.engine, full_fuzzer_name, cross_pollination_method, tag)\n\n  context = Context(fuzz_target, cross_pollinate_fuzzers,\n                    cross_pollination_method, tag)\n\n  # Copy global blacklist into local suppressions file if LSan is enabled.\n  is_lsan_enabled = environment.get_value('LSAN')\n  if is_lsan_enabled:\n    # TODO(ochang): Copy this to untrusted worker.\n    leak_blacklist.copy_global_to_local_blacklist()\n\n  try:\n    result = do_corpus_pruning(context, last_execution_failed, revision)\n    _record_cross_pollination_stats(result.cross_pollination_stats)\n    _save_coverage_information(context, result)\n    _process_corpus_crashes(context, result)\n  except Exception:\n    logs.log_error('Corpus pruning failed.')\n    data_handler.update_task_status(task_name, data_types.TaskState.ERROR)\n    return\n  finally:\n    context.cleanup()\n\n  data_handler.update_task_status(task_name, data_types.TaskState.FINISHED)\n","numLines":960,"startLine":1,"totalLines":960}}}
{"type":"assistant","message":{"model":"claude-opus-4-20250514","id":"msg_01Dx6SXzbsUrByUrxRwgD7bJ","type":"message","role":"assistant","content":[{"type":"text","text":"Now I can see the code. Looking at the `do_corpus_pruning` function (lines 587-717), I can see the `if not build_directory` check on line 600. Let me analyze this check in context:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":6,"cache_creation_input_tokens":14237,"cache_read_input_tokens":8078,"cache_creation":{"ephemeral_5m_input_tokens":14237,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"6939fe31-d677-4c31-9307-4891924f760e","uuid":"db92eff9-9dd9-41f4-a2b4-80ef55a92381"}
{"type":"assistant","message":{"model":"claude-opus-4-20250514","id":"msg_01Dx6SXzbsUrByUrxRwgD7bJ","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01Nx5JkZmeiyqsTYMMCRyAdh","name":"Read","input":{"file_path":"/Users/emmayu/crqbench/projects/clusterfuzz/src/python/bot/tasks/corpus_pruning_task.py","offset":595,"limit":10}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":6,"cache_creation_input_tokens":14237,"cache_read_input_tokens":8078,"cache_creation":{"ephemeral_5m_input_tokens":14237,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"6939fe31-d677-4c31-9307-4891924f760e","uuid":"a82f9790-acf0-44c2-b79a-f869827bec26"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01Nx5JkZmeiyqsTYMMCRyAdh","type":"tool_result","content":"   595→    return tasks_host.do_corpus_pruning(context, last_execution_failed,\n   596→                                        revision)\n   597→\n   598→  build_setup_result = build_manager.setup_build(revision=revision)\n   599→  build_directory = environment.get_value('BUILD_DIR')\n   600→  if not build_setup_result or not build_directory:\n   601→    raise CorpusPruningException('Failed to setup build.')\n   602→\n   603→  start_time = datetime.datetime.utcnow()\n   604→  runner = Runner(build_directory, context)\n\n<system-reminder>\nWhenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.\n</system-reminder>\n"}]},"parent_tool_use_id":null,"session_id":"6939fe31-d677-4c31-9307-4891924f760e","uuid":"693907b1-f774-4892-a753-282b0cfedae3","tool_use_result":{"type":"text","file":{"filePath":"/Users/emmayu/crqbench/projects/clusterfuzz/src/python/bot/tasks/corpus_pruning_task.py","content":"    return tasks_host.do_corpus_pruning(context, last_execution_failed,\n                                        revision)\n\n  build_setup_result = build_manager.setup_build(revision=revision)\n  build_directory = environment.get_value('BUILD_DIR')\n  if not build_setup_result or not build_directory:\n    raise CorpusPruningException('Failed to setup build.')\n\n  start_time = datetime.datetime.utcnow()\n  runner = Runner(build_directory, context)","numLines":10,"startLine":595,"totalLines":960}}}
{"type":"assistant","message":{"model":"claude-opus-4-20250514","id":"msg_01239Ym7E7x4oy2pivWyFdDX","type":"message","role":"assistant","content":[{"type":"text","text":"Looking at this code, the check `if not build_directory` on line 600 **is necessary** for the following reasons:\n\n1. **Defensive programming**: Even though `build_manager.setup_build()` should set the `BUILD_DIR` environment variable when successful, there's no guarantee that the environment variable will be set properly. The check ensures that both conditions are met before proceeding.\n\n2. **build_directory is used immediately after**: On line 604, `build_directory` is passed to the `Runner` constructor. Without this check, if `build_directory` is `None` or empty, the code would fail with a less informative error when trying to use it.\n\n3. **Consistent with build_setup_result check**: The code already checks `build_setup_result`, and checking `build_directory` provides an additional layer of validation that the build setup was truly successful and all required values are available.\n\n4. **Clear error messaging**: By checking both conditions together, the code provides a clear error message \"Failed to setup build\" that covers both potential failure cases.\n\nThe recent commit message \"Make build setup result checking more strict in corpus pruning\" (fb1bf92d) suggests that the developers intentionally want stricter validation in this area, which supports keeping this check.\n\nIn summary, the `if not build_directory` check should be kept as it provides important validation before the variable is used and helps prevent potential runtime errors with better error messaging."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":6,"cache_creation_input_tokens":418,"cache_read_input_tokens":22315,"cache_creation":{"ephemeral_5m_input_tokens":418,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"6939fe31-d677-4c31-9307-4891924f760e","uuid":"32936f00-8dd5-4695-bc77-1abff0e6aeb6"}
{"type":"result","subtype":"success","is_error":false,"duration_ms":56645,"duration_api_ms":55694,"num_turns":11,"result":"Looking at this code, the check `if not build_directory` on line 600 **is necessary** for the following reasons:\n\n1. **Defensive programming**: Even though `build_manager.setup_build()` should set the `BUILD_DIR` environment variable when successful, there's no guarantee that the environment variable will be set properly. The check ensures that both conditions are met before proceeding.\n\n2. **build_directory is used immediately after**: On line 604, `build_directory` is passed to the `Runner` constructor. Without this check, if `build_directory` is `None` or empty, the code would fail with a less informative error when trying to use it.\n\n3. **Consistent with build_setup_result check**: The code already checks `build_setup_result`, and checking `build_directory` provides an additional layer of validation that the build setup was truly successful and all required values are available.\n\n4. **Clear error messaging**: By checking both conditions together, the code provides a clear error message \"Failed to setup build\" that covers both potential failure cases.\n\nThe recent commit message \"Make build setup result checking more strict in corpus pruning\" (fb1bf92d) suggests that the developers intentionally want stricter validation in this area, which supports keeping this check.\n\nIn summary, the `if not build_directory` check should be kept as it provides important validation before the variable is used and helps prevent potential runtime errors with better error messaging.","session_id":"6939fe31-d677-4c31-9307-4891924f760e","total_cost_usd":0.6749557499999999,"usage":{"input_tokens":69,"cache_creation_input_tokens":22733,"cache_read_input_tokens":89622,"output_tokens":1410,"server_tool_use":{"web_search_requests":0,"web_fetch_requests":0},"service_tier":"standard","cache_creation":{"ephemeral_1h_input_tokens":0,"ephemeral_5m_input_tokens":22733}},"modelUsage":{"claude-opus-4-20250514":{"inputTokens":69,"outputTokens":1410,"cacheReadInputTokens":89622,"cacheCreationInputTokens":22733,"webSearchRequests":0,"costUSD":0.66746175,"contextWindow":200000,"maxOutputTokens":32000},"claude-haiku-4-5-20251001":{"inputTokens":6714,"outputTokens":156,"cacheReadInputTokens":0,"cacheCreationInputTokens":0,"webSearchRequests":0,"costUSD":0.007494,"contextWindow":200000,"maxOutputTokens":64000}},"permission_denials":[{"tool_name":"Bash","tool_use_id":"toolu_013tT3EF8gc289db3Nm1esbs","tool_input":{"command":"ls -la *.py","description":"List Python files in current directory"}},{"tool_name":"Bash","tool_use_id":"toolu_01CX8ioHCXA4LEW5yQoiVmtR","tool_input":{"command":"pwd && ls *.py","description":"Show current directory and list Python files"}},{"tool_name":"Bash","tool_use_id":"toolu_01SjH5SvZeY4iRJ32KzLFvrg","tool_input":{"command":"grep -n \"if not build_directory\" *.py","description":"Search for 'if not build_directory' in Python files"}},{"tool_name":"Bash","tool_use_id":"toolu_018TcZKDiZdyZGHHfbRxLy7z","tool_input":{"command":"grep -n \"build_directory\" *.py | grep -v \"^Binary\"","description":"Search for build_directory references in Python files"}}],"uuid":"5919d609-a118-442f-a8fe-bf74e66e81a1"}
